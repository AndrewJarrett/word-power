{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/Cython/Distutils/old_build_ext.py:30: UserWarning: Cython.Distutils.old_build_ext does not properly handle dependencies and is deprecated.\n",
      "  \"Cython.Distutils.old_build_ext does not properly handle dependencies \"\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "from pandas import DataFrame, read_sas, read_csv\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "\n",
    "from SECEdgar.crawler import SecCrawler\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "from datetime import date, timedelta\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import os\n",
    "import re\n",
    "import lxml\n",
    "import redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 1s, sys: 12.2 s, total: 6min 14s\n",
      "Wall time: 6min 14s\n"
     ]
    }
   ],
   "source": [
    "# Read in SAS data set - takes a while...\n",
    "%time data = read_sas(\"data/crsp_comp.sas7bdat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.84 ms, sys: 7.56 ms, total: 14.4 ms\n",
      "Wall time: 16.1 ms\n",
      "CPU times: user 5.09 ms, sys: 15.9 ms, total: 21 ms\n",
      "Wall time: 20.9 ms\n",
      "CPU times: user 2.54 ms, sys: 70.7 ms, total: 73.2 ms\n",
      "Wall time: 73.2 ms\n",
      "CPU times: user 31.1 ms, sys: 110 ms, total: 141 ms\n",
      "Wall time: 140 ms\n"
     ]
    }
   ],
   "source": [
    "# Read in the positive word list\n",
    "%time pos_list = read_csv(\"data/pos_list.csv\", header=None, names=['word'])\n",
    "%time pos_roots = read_csv(\"data/pos_roots.csv\")\n",
    "pos_roots_dict = dict(zip(list(pos_roots.word), list(pos_roots.group)))\n",
    "\n",
    "# Read in the negative word list\n",
    "%time neg_list = read_csv(\"data/neg_list.csv\", header=None, names=['word'])\n",
    "%time neg_roots = read_csv(\"data/neg_roots.csv\")\n",
    "neg_roots_dict = dict(zip(list(neg_roots.word), list(neg_roots.group)))\n",
    "\n",
    "# Turn them into a Series for easier lookups later on\n",
    "pos_list = pos_list.iloc[:]\n",
    "neg_list = neg_list.iloc[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sort the set by cusip, permno, cik, and then year (descending)\n",
    "data.sort_values(['CUSIP', 'PERMNO', 'cik', 'year'], ascending=[True, True, True, False], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove any duplicates where CUSIP, PERMNO, and CIK match\n",
    "ciks = data.drop_duplicates(subset=['CUSIP', 'PERMNO', 'cik'])\n",
    "\n",
    "# Only keep the cik and ticker column\n",
    "ciks = ciks[['cik', 'tic']]\n",
    "\n",
    "# Re-index the dataframe for better access\n",
    "data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Iterate over each CIK and pull the relevant 10k filings\n",
    "crawler = SecCrawler()\n",
    "end_date = '20081231'\n",
    "count = '20'\n",
    "\n",
    "for index, row in ciks.iterrows():\n",
    "    cik = row.iloc[0]\n",
    "    tic = row.iloc[1]\n",
    "    crawler.filing_10K(tic, cik, end_date, count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%%cython\n",
    "import pickle as pickle\n",
    "\n",
    "#cdef void cache_objects(dict objs, int count, int batch):\n",
    "def cache_objects(objs, count, batch):\n",
    "    if count % batch == 0:\n",
    "        print(\"Count: \" + str(count))\n",
    "        for name, obj in objs.items():\n",
    "            print(\"Saving the \" + name + \" object...\")\n",
    "            pickle.dump(obj, open(\"data/\" + name + \".p\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) Processing SEC-Edgar-data/3MCLN./0000789547/10-K/0000948830-96-000019.txt\n",
      "CPU times: user 3.8 s, sys: 7.9 ms, total: 3.81 s\n",
      "Wall time: 3.8 s\n",
      "No stock data found - moved file.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'unicode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-e1ea6edae9a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    180\u001b[0m                     \u001b[0;34m'file_name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                     \u001b[0;34m'filing_date'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfiling_ts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m                     \u001b[0;34m'report'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m                     \u001b[0;34m'index'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                     \u001b[0;34m'mtime'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'unicode' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Pull in one file to start working on the parsing algorithm\n",
    "if filings is None:    \n",
    "    try:\n",
    "        filings = pickle.load(open(\"data/filings.p\", \"rb\"))\n",
    "    except:\n",
    "        filings = DataFrame()\n",
    "    \n",
    "if processed is None:\n",
    "    try:\n",
    "        processed = pickle.load(open(\"data/processed.p\", \"rb\"))    \n",
    "    except:\n",
    "        processed = set()\n",
    "\n",
    "# Batch save the filings info\n",
    "count = 0\n",
    "batch = 1000\n",
    "\n",
    "# This is for testing\n",
    "stop = 10\n",
    "skip_processed = False\n",
    "process = False\n",
    "\n",
    "# Connect to redis\n",
    "rds = redis.Redis()\n",
    "\n",
    "folder = \"SEC-Edgar-data\"\n",
    "for (dirpath, dirnames, filenames) in os.walk(folder, topdown=False):\n",
    "    #print (\"dirpath: \" + dirpath + \"; # files: \" + str(len(filenames)))\n",
    "    for filename in filenames:\n",
    "        \n",
    "        fn = os.sep.join([dirpath, filename])\n",
    "\n",
    "        if filename.endswith('.txt'):\n",
    "            count += 1\n",
    "            if count > stop:\n",
    "                break\n",
    "            \n",
    "            # Skip this file if it exists in the filings object\n",
    "            #if skip_processed and 'path' in filings.columns and fn in filings.path.values:\n",
    "            if skip_processed and fn in processed:\n",
    "                print(\"File already processed: \" + fn + \".\")\n",
    "                continue\n",
    "\n",
    "            print(\"(\" + str(count) + \") Processing \" + fn)\n",
    "            tic = fn.split('/')[1]\n",
    "            cik = fn.split('/')[2]\n",
    "            \n",
    "            # Pull in 10-K from redis if it exists\n",
    "            key = \"report:\" + cik + \":\" + fn\n",
    "            exists = rds.exists(key)\n",
    "            \n",
    "            # If the key exists, then we can skip to text processing\n",
    "            if not exists or not skip_processed:\n",
    "                # Open the file, get all of the content, and then pull it into a parser\n",
    "                fh = open(fn, 'r')\n",
    "                contents = fh.read()\n",
    "\n",
    "                # Clean up some of the text to fix malformed HTML before parsing it\n",
    "                malformed_tags = ['ACCEPTANCE-DATETIME', 'TYPE', 'SEQUENCE', 'FILENAME', 'DESCRIPTION']\n",
    "                for tag in malformed_tags:\n",
    "                    # Do a regex that replaces all of these malformed tags in the document\n",
    "                    regex = re.compile(r\"(\\n<%s>[^<]*?)\\n\" % re.escape(tag), re.I)\n",
    "                    contents = regex.sub(r\"\\1</%s>\\n\" % tag, contents)\n",
    "\n",
    "                # Pull the 10-k into the parser\n",
    "                document = bs(contents, 'lxml')\n",
    "\n",
    "                # The document can either have a root node of sec-document or ims-document\n",
    "                if document.find('sec-document') is not None:\n",
    "                    root = document.find('sec-document')\n",
    "                elif document.find('ims-document') is not None: \n",
    "                    root = document.find('ims-document')\n",
    "                else:\n",
    "                    root = document.find('document')\n",
    "\n",
    "                # Check if this is an amended 10-K and throw it out if so\n",
    "                type_text = root.find('type')\n",
    "                if type_text is None:\n",
    "                    # Couldn't find the type so we move it to the _error folder\n",
    "                    new_name = 'data/_error/' + tic + '-' + cik + '-' + filename\n",
    "\n",
    "                    # Close the file so that we can move it\n",
    "                    fh.close()\n",
    "                    os.rename(fn, new_name)\n",
    "                    print(\"Error finding type - moved file\")\n",
    "                    continue\n",
    "\n",
    "                elif type_text.text == '10-K/A':\n",
    "                    # This is an amended 10-k, move it to the \"data/_amended\" folder\n",
    "                    new_name = 'data/_amended/' + tic + '-' + cik + '-' + filename\n",
    "\n",
    "                    # Close the file so that we can move it\n",
    "                    fh.close()\n",
    "                    os.rename(fn, new_name)\n",
    "                    print(\"Amended 10-K - moved file\")\n",
    "                    continue\n",
    "\n",
    "                # Get the 'acceptance-datetime' metadata element\n",
    "                acc_dt = root.find('acceptance-datetime')\n",
    "                if acc_dt is None:\n",
    "                    header_text = None\n",
    "                    # If we didn't find an <acceptance-datetime /> element, find the date elsewhere\n",
    "                    if root.find('sec-header') is not None:\n",
    "                        header_text = root.find('sec-header').text\n",
    "                    elif root.find('ims-header') is not None:\n",
    "                        header_text = root.find('ims-header').text\n",
    "\n",
    "                    if header_text:\n",
    "                        regex = re.compile(r\".*\\nFILED AS OF DATE:\\s+?([\\d]+?)\\n.*\", re.S)\n",
    "                        filing_dt_text = re.sub(regex, r\"\\1\", header_text)\n",
    "                    else:\n",
    "                        # We can't find the filing date for this file so throw it out\n",
    "                        new_name = 'data/_error/' + tic + '-' + cik + '-' + filename\n",
    "\n",
    "                        # Close the file so that we can move it\n",
    "                        fh.close()\n",
    "                        os.rename(fn, new_name)\n",
    "                        print(\"Bad filing date - moved file\")\n",
    "                        continue\n",
    "                else:\n",
    "                    # Get the filing date\n",
    "                    filing_dt_text = acc_dt.text.split('\\n', 1)[0][:8]\n",
    "\n",
    "                filing_dt = dt.strptime(filing_dt_text, '%Y%m%d')\n",
    "                filing_ts = time.mktime(filing_dt.timetuple())\n",
    "                begin_dt = dt(1995, 1, 1)\n",
    "\n",
    "                # If the filing date is not within our date range, then move it\n",
    "                if begin_dt > filing_dt:\n",
    "                    # This file is outside of our date range so move it\n",
    "                    new_name = 'data/_outofrange/' + tic + '-' + cik + '-' + filename\n",
    "\n",
    "                    # Close the file so that we can move it\n",
    "                    fh.close()\n",
    "                    os.rename(fn, new_name)\n",
    "                    print(\"Out of date range - moved file.\")\n",
    "                    continue\n",
    "                    \n",
    "                # See if we can find stock info for this company on the filing date of the 10-K\n",
    "                try:\n",
    "                    #pass\n",
    "                    %time cik_df = data[(data['cik'] == bytes(cik, 'utf-8'))]\n",
    "                    %time index = cik_df[(cik_df['date'] == filing_dt)].index[0]\n",
    "                    #index = data[(data['cik'] == bytes(cik, 'utf-8')) & (data['date'] == filing_dt)].index[0]\n",
    "                except:\n",
    "                    # We don't have stock data for this company at this time period\n",
    "                    new_name = 'data/_nostockdata/' + tic + '-' + cik + '-' + filename\n",
    "\n",
    "                    # Close the file so that we can move it\n",
    "                    fh.close()\n",
    "                    #os.rename(fn, new_name)\n",
    "                    print(\"No stock data found - moved file.\")\n",
    "                    #continue\n",
    "\n",
    "                # Remove the exhibits\n",
    "                [ex.extract() for ex in root.findAll('document')[1:]]\n",
    "\n",
    "                # Grab the report (and throw out images, tables)\n",
    "                report = root.find('text')\n",
    "\n",
    "                # Remove some elements\n",
    "                del_tags = ['img', 'hr', 'head']\n",
    "                [t.extract() for t in report.findAll(del_tags)]\n",
    "\n",
    "                strip_tags = ['b', 'i', 'u', 'sup', 'em', 'strong', 'font', 'p', 'div', 'td', 'tr', 'table', 'body', 'html', 'page', 'text']\n",
    "                [t.replaceWithChildren() for t in report.findAll(strip_tags)]\n",
    "\n",
    "                replace_tags = [{'br': '\\n'}]\n",
    "                for tag in replace_tags:\n",
    "                    tag, replace = tag.popitem()\n",
    "                    [t.replaceWith(replace) for t in report.findAll(tag)]\n",
    "\n",
    "                # Save the text in redis\n",
    "                report_hash = {\n",
    "                    'cik': cik,\n",
    "                    'tic': tic,\n",
    "                    'path': fn,\n",
    "                    'file_name': filename,\n",
    "                    'filing_date': filing_ts,\n",
    "                    'report': pickle.dumps(unicode(report)),\n",
    "                    'index': index,\n",
    "                    'mtime': time.time()\n",
    "                }\n",
    "                print(\"Saving to redis: \" + key)\n",
    "                rds.hmset(key, report_hash)\n",
    "                \n",
    "                # Close the file handle\n",
    "                fh.close()\n",
    "\n",
    "                # If we don't want to process the file, then we will quit here\n",
    "                if not process:\n",
    "                    # Save this file as processed\n",
    "                    processed.add(fn)\n",
    "\n",
    "                    # Save certain objects so we don't have to process everything again\n",
    "                    objs = {'filings': filings, 'processed': processed}\n",
    "                    cache_objects(objs, count, batch)\n",
    "                    continue\n",
    "            else:\n",
    "                # Get the report out of redis\n",
    "                print(\"Found in redis: \" + key)\n",
    "                report = pickle.loads(rds.hget(key, 'report'))\n",
    "            \n",
    "            # Now that everything is cleaned up, we can run the word processing algorithm\n",
    "            pos_occurs = defaultdict(int)\n",
    "            neg_occurs = defaultdict(int)\n",
    "            negators = pd.Series(['not', 'no', 'never'])\n",
    "\n",
    "            # We will tokenize the text and iterate through each word\n",
    "            tokens = pd.Series(report.text.split())\n",
    "            total_words = len(tokens)\n",
    "\n",
    "            # First, filter out words that aren't in the 12dictionary word list\n",
    "\n",
    "            # Now, process the text\n",
    "            for index, token in tokens.iteritems():\n",
    "                if token in pos_list.values:\n",
    "                    # Check to see if there is a negator\n",
    "                    negated = False\n",
    "                    for word in tokens.iloc[(index - 3):(index + 3)]:\n",
    "                        if word in negators.values:\n",
    "                            #print(\"Found a negator: \" + word + \" - \" + token)\n",
    "                            negated = True\n",
    "\n",
    "                    if not negated:\n",
    "                        root = pos_roots_dict[token]\n",
    "                        pos_occurs[root] += 1\n",
    "                elif token in neg_list.values:\n",
    "                    # Check to see if there is a negator\n",
    "                    negated = False\n",
    "                    for word in tokens.iloc[(index - 3):(index + 3)]:\n",
    "                        if word in negators.values:\n",
    "                            #print(\"Found a negator: \" + word + \" - \" + token)\n",
    "                            negated = True\n",
    "\n",
    "                    if not negated:\n",
    "                        root = neg_roots_dict[token]\n",
    "                        neg_occurs[root] += 1\n",
    "\n",
    "            # Save results of text processing to key in redis            \n",
    "            report_hash = {\n",
    "                'pos_occurs': pickle.dumps(pos_occurs),\n",
    "                'neg_occurs': pickle.dumps(neg_occurs),\n",
    "                'total_words': total_words,\n",
    "                'mtime': time.time()\n",
    "            }\n",
    "            rds.hmset(key, report_hash)\n",
    "            \n",
    "            # Add the info for this 10-K to the filings dataframe to keep track of it\n",
    "            report_hash['pos_occurs'] = pos_occurs\n",
    "            report_hash['neg_occurs'] = neg_occurs\n",
    "            filings = filings.append(report_hash, ignore_index=True)\n",
    "            \n",
    "            # Use the index we found earlier to ensure we have stock data and now we grab the four day window returns\n",
    "            returns = data.ix[index:(index + 3)]\n",
    "            returns = returns[['RET', 'vwretd']]\n",
    "\n",
    "            # Calculate the abnormal return: r_i = M{t=0, 3} (ret_i,j) - M{t=0,3} (ret_vwi,t)\n",
    "            ret = 1\n",
    "            ret_vwi = 1\n",
    "            for col, series in returns.iteritems():\n",
    "                if col == 'RET':\n",
    "                    for r in series:\n",
    "                        ret *= r\n",
    "                elif col == 'vwretd':\n",
    "                    for r in series:\n",
    "                        ret_vwi *= r\n",
    "            ab_ret = ret - ret_vwi\n",
    "            print(\"Abnormal return: \" + str(ab_ret))\n",
    "            \n",
    "            # Now estimate the weights for the words\n",
    "            \n",
    "            \n",
    "            # Normalize the weights of the words\n",
    "            \n",
    "            \n",
    "            # Run the actual regression using the estimated weights for the words\n",
    "            \n",
    "            \n",
    "            # Calculate the score of the document using the weights for each word given by the regression\n",
    "            \n",
    "            \n",
    "            # Save this file as processed\n",
    "            processed.add(fn)\n",
    "\n",
    "            # Save certain objects so we don't have to process everything again\n",
    "            objs = {'filings': filings, 'processed': processed}\n",
    "            cache_objects(objs, count, batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-1.2158094036836717e-07, -7.2910937480136238e-10, -1.208518309935658e-07)\n"
     ]
    }
   ],
   "source": [
    "index = data[(data['cik'] == b'0000320193') & (data['date'] == filing_dt)].index[0]\n",
    "returns = data.ix[index:(index + 3)]\n",
    "returns = returns[['RET', 'vwretd']]\n",
    "\n",
    "ret = 1\n",
    "ret_vwi = 1\n",
    "\n",
    "for col, series in returns.iteritems():\n",
    "    if col == 'RET':\n",
    "        for r in series:\n",
    "            #print((ret,r))\n",
    "            ret *= r\n",
    "    elif col == 'vwretd':\n",
    "        for r in series:\n",
    "            #print((ret_vwi,r))\n",
    "            ret_vwi *= r\n",
    "\n",
    "ab_ret = ret - ret_vwi\n",
    "print((ret, ret_vwi, ab_ret))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RET</th>\n",
       "      <th>vwretd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1024959</th>\n",
       "      <td>-0.037838</td>\n",
       "      <td>0.015756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024960</th>\n",
       "      <td>0.056180</td>\n",
       "      <td>0.003094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024961</th>\n",
       "      <td>-0.010638</td>\n",
       "      <td>-0.003173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024962</th>\n",
       "      <td>-0.005376</td>\n",
       "      <td>0.004714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              RET    vwretd\n",
       "1024959 -0.037838  0.015756\n",
       "1024960  0.056180  0.003094\n",
       "1024961 -0.010638 -0.003173\n",
       "1024962 -0.005376  0.004714"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns[['RET', 'vwretd']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Connect to redis\n",
    "rds = redis.Redis()\n",
    "\n",
    "# This will store the global positive and negative words occurrances\n",
    "pos_occurs_all = defaultdict(int)\n",
    "neg_occurs_all = defaultdict(int)\n",
    "\n",
    "# Go through redis and grab every 10-k\n",
    "keys = rds.keys(\"report:*\")\n",
    "\n",
    "print(\"Total keys: \" + str(len(keys)))\n",
    "for key in keys:\n",
    "    report_pos_occurs = pickle.loads(rds.hget(key, 'pos_occurs'))\n",
    "    report_neg_occurs = pickle.loads(rds.hget(key, 'neg_occurs'))\n",
    "    \n",
    "    for word, freq in report_pos_occurs.items():\n",
    "        pos_occurs_all[word] += freq\n",
    "        \n",
    "    for word, freq in report_neg_occurs.items():\n",
    "        neg_occurs_all[word] += freq\n",
    "\n",
    "# Print out most frequent positive words\n",
    "print(\"Most Frequent Positive Words\\n\" +\n",
    "       \"============================\")\n",
    "\n",
    "pos_sorted = pd.Series(data=pos_occurs_all).sort_values(ascending=False)\n",
    "print(pos_sorted)\n",
    "\n",
    "# Print out most frequent negative words\n",
    "print(\"\\n\\nMost Frequent Negative Words\\n\" +\n",
    "       \"============================\")\n",
    "\n",
    "neg_sorted = pd.Series(data=neg_occurs_all).sort_values(ascending=False)\n",
    "print(neg_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%prun -l 4 cache_objects({'test': processed, 'test2': DataFrame()}, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'0000320193'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "objs = {'filings': filings, 'processed': processed}\n",
    "for name, obj in objs.iteritems():\n",
    "    print (name, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
