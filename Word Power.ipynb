{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from pandas import DataFrame, read_sas, read_csv\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from SECEdgar.crawler import SecCrawler\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "from datetime import date\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import os\n",
    "import re\n",
    "import lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 10s\n"
     ]
    }
   ],
   "source": [
    "# Read in SAS data set - takes a while...\n",
    "%time data = read_sas(\"data/crsp_comp.sas7bdat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5 ms\n",
      "Wall time: 15 ms\n",
      "Wall time: 8 ms\n",
      "Wall time: 9 ms\n"
     ]
    }
   ],
   "source": [
    "# Read in the positive word list\n",
    "%time pos_list = read_csv(\"data/pos_list.csv\", header=None, names=['word'])\n",
    "%time pos_roots = read_csv(\"data/pos_roots.csv\")\n",
    "pos_roots_dict = dict(zip(list(pos_roots.word), list(pos_roots.group)))\n",
    "\n",
    "# Read in the negative word list\n",
    "%time neg_list = read_csv(\"data/neg_list.csv\", header=None, names=['word'])\n",
    "%time neg_roots = read_csv(\"data/neg_roots.csv\")\n",
    "neg_roots_dict = dict(zip(list(neg_roots.word), list(neg_roots.group)))\n",
    "\n",
    "# Turn them into a Series for easier lookups later on\n",
    "pos_list = pos_list.iloc[:]\n",
    "neg_list = neg_list.iloc[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "718"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neg_roots.group.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'CUSIP' u'PERMNO' u'cik' u'date' u'year' u'fyear' u'tic' u'conm'\n",
      " u'mkvalt' u'at' u'intan' u'lt' u'book_market' u'tlta' u'cacl' u'nita'\n",
      " u'icf_na' u'rsiz' u'mkt_val' u'PRC' u'RET' u'ASKHI' u'BIDLO' u'VOL'\n",
      " u'RETX' u'vwretd' u'totval']\n",
      "['word']\n",
      "['word']\n"
     ]
    }
   ],
   "source": [
    "# List column names\n",
    "print(data.columns.values)\n",
    "print(pos_list.columns.values)\n",
    "print(neg_list.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sort the set by cusip, permno, cik, and then year (descending)\n",
    "data.sort_values(['CUSIP', 'PERMNO', 'cik', 'year'], ascending=[True, True, True, False], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove any duplicates where CUSIP, PERMNO, and CIK match\n",
    "ciks = data.drop_duplicates(subset=['CUSIP', 'PERMNO', 'cik'])\n",
    "\n",
    "# Only keep the cik and ticker column\n",
    "ciks = ciks[['cik', 'tic']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Iterate over each CIK and pull the relevant 10k filings\n",
    "crawler = SecCrawler()\n",
    "end_date = '20081231'\n",
    "count = '20'\n",
    "\n",
    "for index, row in ciks.iterrows():\n",
    "    cik = row.iloc[0]\n",
    "    tic = row.iloc[1]\n",
    "    crawler.filing_10K(tic, cik, end_date, count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pull in one file to start working on the parsing algorithm\n",
    "filings = DataFrame()\n",
    "\n",
    "fn = 'SEC-Edgar-data/3ABHH/0001052489/10-K/0001193125-07-096277.txt'\n",
    "tic = fn.split('/')[1]\n",
    "cik = fn.split('/')[2]\n",
    "file_name = fn.split('/')[4]\n",
    "\n",
    "# Open the file, get all of the content, and then pull it into a parser\n",
    "fh = open(fn, 'r')\n",
    "contents = fh.read()\n",
    "\n",
    "# Clean up some of the text to fix malformed HTML before parsing it\n",
    "malformed_tags = ['ACCEPTANCE-DATETIME', 'TYPE', 'SEQUENCE', 'FILENAME', 'DESCRIPTION']\n",
    "for tag in malformed_tags:\n",
    "    # Do a regex that replaces all of these malformed tags in the document\n",
    "    regex = re.compile(r\"(\\n<%s>[^<]*?\\n)\" % re.escape(tag), re.I)\n",
    "    contents = regex.sub(r\"\\1</%s>\" % tag, contents)\n",
    "\n",
    "# Pull the 10-k into the parser\n",
    "document = bs(contents, 'lxml')\n",
    "\n",
    "# Check if this is an amended 10-K and throw it out if so\n",
    "type_text = document.find('sec-document').document.type.text\n",
    "if type_text == '10-K/A':\n",
    "    # This is an amended 10-k, move it to the \"SEC-Edgar-data/_amended\" folder\n",
    "    new_name = 'SEC-Edgar-data/_amended/' + tic + '-' + cik + '-' + file_name\n",
    "    \n",
    "    # Close the file so that we can move it\n",
    "    fh.close()\n",
    "    #os.rename(fn, new_name)\n",
    "    print(\"TODO: Amended 10-K - move the file.\")\n",
    "    \n",
    "# Get the 'acceptance-datetime' metadata element\n",
    "acc_dt = document.find('acceptance-datetime')\n",
    "if acc_dt is not None:\n",
    "    # If we didn't find an <acceptance-datetime /> element, find the date elsewhere\n",
    "    header_text = document.find('sec-header').text\n",
    "    regex = re.compile(r\".*\\nFILED AS OF DATE:\\s+?([\\d]+?)\\n.*\", re.S)\n",
    "    filing_dt_text = re.sub(regex, r\"\\1\", header_text)\n",
    "else:\n",
    "    # Get the filing date\n",
    "    filing_dt_text = acc_dt.text.split('\\n', 1)[0]\n",
    "    \n",
    "begin_dt = dt(1995, 1, 1)\n",
    "filing_dt = dt.strptime(filing_dt_text, '%Y%m%d')\n",
    "filing_ts = time.mktime(filing_dt.timetuple())\n",
    "\n",
    "# If the filing date is not within our date range, then move it\n",
    "if begin_dt > filing_dt:\n",
    "    # TODO: Move the file to another folder\n",
    "    print(\"TODO: Out of date range - move the file to another folder\")    \n",
    "\n",
    "# Add the info for this 10-K to the filings dataframe to keep track of it\n",
    "filings = filings.append({'cik': cik, 'tic': tic, 'file_name': file_name, 'filing_date': filing_ts}, ignore_index=True)\n",
    "\n",
    "# Remove the exhibits\n",
    "#for doc in document.find('sec-document').findAll('document'):\n",
    "    #[ex.extract() for ex in docume]\n",
    "\n",
    "# Grab the report (and throw out images, tables)\n",
    "report = document.find('sec-document').document.find('text')\n",
    "\n",
    "# Remove some elements\n",
    "del_tags = ['img', 'hr', 'head']\n",
    "for tag in del_tags:\n",
    "    [t.extract() for t in report.findAll(tag)]\n",
    "\n",
    "strip_tags = ['b', 'i', 'u', 'sup', 'em', 'strong', 'font', 'p', 'div', 'td', 'tr', 'table', 'body', 'html']\n",
    "for tag in strip_tags:\n",
    "    [t.replaceWithChildren() for t in report.findAll(tag)]\n",
    "\n",
    "replace_tags = [{'br': '\\n'}]\n",
    "for tag in replace_tags:\n",
    "    tag, replace = tag.popitem()\n",
    "    [t.replaceWith(replace) for t in report.findAll(tag)]\n",
    "    \n",
    "#print(report.text)\n",
    "\n",
    "# Now that everything is cleaned up, we can run the word processing algorithm\n",
    "pos_occurs = defaultdict(int)\n",
    "neg_occurs = defaultdict(int)\n",
    "negators = ['not', 'no', 'never']\n",
    "\n",
    "# We will tokenize the text and iterate through each word\n",
    "tokens = report.text.split()\n",
    "for index, token in enumerate(tokens):\n",
    "    #print(token)\n",
    "    #print(index, token)\n",
    "    if token in pos_list.values:\n",
    "        #print(\"Positive: \" + token)\n",
    "        # Check to see if there is a negator\n",
    "        negated = False\n",
    "        for word in tokens[(index - 3):(index + 3)]:\n",
    "            if word in negators:\n",
    "                #print(\"Found a negator: \" + word + \" - \" + token)\n",
    "                negated = True\n",
    "                \n",
    "        if not negated:\n",
    "            root = pos_roots_dict[token]\n",
    "            pos_occurs[root] += 1\n",
    "    elif token in neg_list.values:\n",
    "        #print(\"Negative: \" + token)\n",
    "        # Check to see if there is a negator\n",
    "        negated = False\n",
    "        for word in tokens[(index - 3):(index + 3)]:\n",
    "            if word in negators:\n",
    "                #print(\"Found a negator: \" + word + \" - \" + token)\n",
    "                negated = True\n",
    "                \n",
    "        if not negated:\n",
    "            root = neg_roots_dict[token]\n",
    "            neg_occurs[root] += 1\n",
    "\n",
    "#print(pos_occurs, neg_occurs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Frequent Positive Words\n",
      "beneficial:\t43\n",
      "outstanding:\t10\n",
      "opportunities:\t8\n",
      "good:\t7\n",
      "achieve:\t5\n",
      "reward:\t3\n",
      "succeed:\t2\n",
      "strength:\t2\n",
      "effective:\t2\n",
      "attain:\t2\n",
      "valuable:\t1\n",
      "satisfactorily:\t1\n",
      "highest:\t1\n",
      "gain:\t1\n",
      "exclusive:\t1\n",
      "enable:\t1\n"
     ]
    }
   ],
   "source": [
    "print(\"Most Frequent Positive Words\")\n",
    "pos_sorted = pd.Series(data=pos_occurs).sort_values(ascending=False)\n",
    "for key, freq in pos_sorted.iteritems():\n",
    "    print(key + \":\\t\" + str(freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Frequent Negative Words\n",
      "terminations:\t36\n",
      "resigns:\t15\n",
      "failures:\t9\n",
      "against:\t7\n",
      "lapsing:\t6\n",
      "forfeitures:\t6\n",
      "restating:\t5\n",
      "omitting:\t5\n",
      "breaching:\t5\n",
      "ceasing:\t4\n",
      "dispose:\t4\n",
      "adversity:\t3\n",
      "standstills:\t3\n",
      "disregards:\t2\n",
      "closures:\t2\n",
      "disclaims:\t2\n",
      "fraudulently:\t2\n",
      "inconsistently:\t2\n",
      "bankrupts:\t2\n",
      "oppositions:\t2\n",
      "convictions:\t2\n",
      "critically:\t1\n",
      "embezzling:\t1\n",
      "delinquents:\t1\n",
      "denying:\t1\n",
      "disclosing:\t1\n",
      "foregone:\t1\n",
      "limitations:\t1\n",
      "liquidators:\t1\n",
      "refusing:\t1\n",
      "absences:\t1\n"
     ]
    }
   ],
   "source": [
    "print(\"Most Frequent Negative Words\")\n",
    "neg_sorted = pd.Series(data=neg_occurs).sort_values(ascending=False)\n",
    "for key, freq in neg_sorted.iteritems():\n",
    "    print(key + \":\\t\" + str(freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
