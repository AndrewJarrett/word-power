{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/Cython/Distutils/old_build_ext.py:30: UserWarning: Cython.Distutils.old_build_ext does not properly handle dependencies and is deprecated.\n",
      "  \"Cython.Distutils.old_build_ext does not properly handle dependencies \"\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame, read_sas, read_csv\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from SECEdgar.crawler import SecCrawler\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "from datetime import date, timedelta\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import os\n",
    "import re\n",
    "import lxml\n",
    "import redis\n",
    "import string\n",
    "import pickle\n",
    "import math\n",
    "import zlib\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "try: stopwords.words('english')\n",
    "except LookupError: nltk.download('stopwords')\n",
    "    \n",
    "import statsmodels.api as sm\n",
    "    \n",
    "rds = redis.Redis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 34s, sys: 13.9 s, total: 6min 48s\n",
      "Wall time: 6min 51s\n"
     ]
    }
   ],
   "source": [
    "# Read in SAS data set - takes a while so try to use redis...\n",
    "try: data\n",
    "except NameError:\n",
    "    if rds.exists('data:word-power'):\n",
    "        %time data = pickle.loads(zlib.decompress(rds.get('data:word-power')))\n",
    "    else:\n",
    "        %time data = read_sas(\"data/crsp_comp.sas7bdat\")\n",
    "\n",
    "        # Trim the SAS data set\n",
    "        data = data[['CUSIP','PERMNO','cik','date','PRC','RET','vwretd']]\n",
    "\n",
    "        # Sort the set by cusip, permno, cik, and then year (descending)\n",
    "        data.sort_values(['CUSIP', 'PERMNO', 'cik', 'date'], ascending=[True, True, True, False], inplace=True)\n",
    "\n",
    "        # Re-index the dataframe for better access\n",
    "        data.reset_index(inplace=True)\n",
    "\n",
    "        rds.set('data:word-power', zlib.compress(pickle.dumps(data)))\n",
    "\n",
    "# We only need certain columns from the data set and we must set the right index for performance\n",
    "try: df\n",
    "except:\n",
    "    df = data[[\"cik\", \"date\", \"PRC\", \"RET\", \"vwretd\"]]\n",
    "    df.set_index(keys=['cik','date'], inplace=True)\n",
    "        \n",
    "# Positive words\n",
    "try: pos_dict, pos_roots, pos_roots_map\n",
    "except:\n",
    "    if rds.exists('data:pos-dict') and rds.exists('data:pos-roots') and rds.exists('data:pos-roots-map'):\n",
    "        pos_dict = pickle.loads(zlib.decompress(rds.get('data:pos-dict')))\n",
    "        pos_roots = pickle.loads(zlib.decompress(rds.get('data:pos-roots')))\n",
    "        pos_roots_map = pickle.loads(zlib.decompress(rds.get('data:pos-roots-map')))\n",
    "    else:\n",
    "        # Read in the positive word list(s)\n",
    "        pos_dict = read_csv(\"data/pos_list.csv\", header=None, names=['word'])\n",
    "        pos_dict = set(pos_dict['word'])\n",
    "        pos_roots = read_csv(\"data/pos_roots.csv\")\n",
    "        pos_roots_map = dict(zip(list(pos_roots.word), list(pos_roots.group)))\n",
    "        pos_roots = set(pos_roots['group'].drop_duplicates())\n",
    "\n",
    "        # Save this data to redis for later\n",
    "        rds.set('data:pos-dict', zlib.compress(pickle.dumps(pos_dict)))\n",
    "        rds.set('data:pos-roots', zlib.compress(pickle.dumps(pos_roots)))\n",
    "        rds.set('data:pos-roots-map', zlib.compress(pickle.dumps(pos_roots_map)))\n",
    "\n",
    "# Negative words\n",
    "try: neg_dict, neg_roots, neg_roots_map\n",
    "except:\n",
    "    if rds.exists('data:neg-dict') and rds.exists('data:neg-roots') and rds.exists('data:neg-roots-map'):\n",
    "        neg_dict = pickle.loads(zlib.decompress(rds.get('data:neg-dict')))\n",
    "        neg_roots = pickle.loads(zlib.decompress(rds.get('data:neg-roots')))\n",
    "        neg_roots_map = pickle.loads(zlib.decompress(rds.get('data:neg-roots-map')))\n",
    "    else:\n",
    "        # Read in the negative word list(s)\n",
    "        neg_dict = read_csv(\"data/neg_list.csv\", header=None, names=['word'])\n",
    "        neg_dict = set(neg_dict['word'])\n",
    "        neg_roots = read_csv(\"data/neg_roots.csv\")\n",
    "        neg_roots_map = dict(zip(list(neg_roots.word), list(neg_roots.group)))\n",
    "        neg_roots = set(neg_roots['group'].drop_duplicates())\n",
    "\n",
    "        # Save this data to redis for later\n",
    "        rds.set('data:neg-dict', zlib.compress(pickle.dumps(neg_dict)))\n",
    "        rds.set('data:neg-roots', zlib.compress(pickle.dumps(neg_roots)))\n",
    "        rds.set('data:neg-roots-map', zlib.compress(pickle.dumps(neg_roots_map)))\n",
    "\n",
    "# 2of12inf dictionary\n",
    "try: dict_2of12inf\n",
    "except:\n",
    "    if rds.exists('data:2of12inf'):\n",
    "        dict_2of12inf = pickle.loads(zlib.decompress(rds.get('data:2of12inf')))\n",
    "    else:\n",
    "        # Read in the 2of12inf\n",
    "        dict_2of12inf = read_csv(\"data/2of12inf.txt\", header=None, names=['word'])\n",
    "\n",
    "        # Iterate through and remove the percent signs\n",
    "        regex = re.compile(r'%$')\n",
    "        dict_2of12inf.apply(lambda x: re.sub(regex, r'', x['word']), axis=1)\n",
    "        dict_2of12inf = set(dict_2of12inf['word'])\n",
    "\n",
    "        # Save this to redis for later\n",
    "        rds.set('data:2of12inf', zlib.compress(pickle.dumps(dict_2of12inf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del data, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Remove any duplicates where CUSIP, PERMNO, and CIK match\n",
    "ciks = data.drop_duplicates(subset=['CUSIP', 'PERMNO', 'cik'])\n",
    "\n",
    "# Only keep the cik and ticker column\n",
    "ciks = ciks[['cik', 'tic']]\n",
    "\n",
    "# Iterate over each CIK and pull the relevant 10k filings\n",
    "crawler = SecCrawler()\n",
    "end_date = '20081231'\n",
    "count = '20'\n",
    "\n",
    "for index, row in ciks.iterrows():\n",
    "    cik = row.iloc[0]\n",
    "    tic = row.iloc[1]\n",
    "    crawler.filing_10K(tic, cik, end_date, count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "from cpython cimport bool\n",
    "\n",
    "from __main__ import rds\n",
    "\n",
    "def check_redis(str cleaned_key, str processed_key, str report_key):\n",
    "    cdef bool processed = False\n",
    "    cdef bool cleaned = False\n",
    "    cdef str mtime\n",
    "    \n",
    "    if not rds.exists(cleaned_key):\n",
    "        if not rds.exists(processed_key):\n",
    "            # Temporary check to see if this file has been processed fully\n",
    "            if rds.exists(report_key):\n",
    "                mtime = rds.hget(report_key, 'mtime')\n",
    "\n",
    "                if not rds.hexists(report_key, 'company_data'):\n",
    "                    # Hasn't been cleaned with the new algorithm, so keep booleans False\n",
    "                    pass\n",
    "                elif rds.hexists(report_key, 'hist_ret'):\n",
    "                    processed = True\n",
    "                    cleaned = True\n",
    "\n",
    "                    # Save to proper place in redis\n",
    "                    rds.set(cleaned_key, mtime)\n",
    "                    rds.set(processed_key, mtime)\n",
    "                else:\n",
    "                    cleaned = True\n",
    "\n",
    "                    # Save to proper place in redis\n",
    "                    rds.set(cleaned_key, mtime)\n",
    "        else:\n",
    "            processed = True\n",
    "    else:\n",
    "        # Check to see if this has really been cleaned (company_data exists)\n",
    "        if rds.hexists(report_key, 'company_data'):\n",
    "            cleaned = True\n",
    "            if rds.exists(processed_key):\n",
    "                processed = True\n",
    "        \n",
    "    return (cleaned, processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "import os\n",
    "\n",
    "def move_file(fh, str fn, str folder, str tic, str cik, str filename, str message):\n",
    "    # Generate the new name of the file\n",
    "    cdef str s = os.sep\n",
    "    cdef str new_name = 'data' + s + folder + s + tic + '-' + cik + '-' + filename\n",
    "\n",
    "    # Close the file so that we can move it\n",
    "    fh.close()\n",
    "    os.rename(fn, new_name)\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name df",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-25a388ec4f31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cython'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'import os\\nimport re\\nimport time\\nimport string\\nimport pickle\\nfrom datetime import datetime as dt\\nfrom lxml import etree\\nfrom io import StringIO\\n\\nfrom bs4 import BeautifulSoup as bs\\n\\nfrom __main__ import move_file, df, stopwords, rds, dict_2of12inf\\n\\nfrom cpython cimport bool\\n\\nregex = re.compile(r\"(.*\\\\.sgml\\\\s+?:\\\\s+?|.*\\\\nFILED AS OF DATE:\\\\s+?)([\\\\d]+?)\\\\n.*\", re.S)\\n\\n# This function handles the cleaning of the 10-K\\ndef clean(str fn):\\n    cdef bool error = False\\n    \\n    cdef str s = os.sep\\n    cdef str tic = fn.split(s)[1]\\n    cdef str cik = fn.split(s)[2]\\n    cdef str filename = fn.split(s)[4]\\n    cdef str report_key = \"report:\" + cik + \":\" + fn\\n    cdef str cleaned_key = \"cleaned:\" + cik + \":\" + fn\\n\\n    # Open the file, get all of the content, and then pull it into a parser\\n    fh = open(fn, \\'r\\')\\n    cdef unicode contents = fh.read()\\n\\n    # Clean up some of the text to fix malformed HTML before parsing it\\n    cdef list malformed_tags = [\\'ACCEPTANCE-DATETIME\\', \\'TYPE\\', \\'SEQUENCE\\', \\'FILENAME\\', \\'DESCRIPTION\\']\\n    cdef str tag\\n    for tag in malformed_tags:\\n        # Do a regex that replaces all of these malformed tags in the document\\n        regex = re.compile(r\"(\\\\n<%s>[^<]*?)\\\\n\" % re.escape(tag), re.I)\\n        contents = regex.sub(r\"\\\\1</%s>\\\\n\" % tag, contents)\\n\\n    # Create the parser\\n    parser = etree.HTMLParser()\\n    document = etree.parse(StringIO(contents), parser)\\n    doc = document.getroot()\\n    \\n    # The document can either have a root node of sec-document or ims-document\\n    if doc.xpath(\\'//sec-document\\') is not None:\\n        root = doc.xpath(\\'//sec-document[1]\\')[0]\\n    elif doc.xpath(\\'//ims-document\\') is not None: \\n        root = doc.xpath(\\'//ims-document[1]\\')[0]\\n    elif doc.xpath(\\'//document\\') is not None:\\n        root = doc.xpath(\\'//document[1]\\')[0]\\n    elif doc.xpath(\\'//error\\') is not None:\\n        root = None\\n    else:\\n        root = None\\n        \\n    if root is None:\\n        # Root node error \\n        move_file(fh, fn, \"_error\", tic, cik, filename, \"No root or erroneous root node - moved file\")\\n        error = True\\n    if error: return error\\n\\n    # Check if this is an amended 10-K and throw it out if so\\n    type_text = root.xpath(\\'//type/text()\\')\\n    if type_text is None or len(type_text) == 0:\\n        move_file(fh, fn, \"_error\", tic, cik, filename, \"Error finding type - moved file\")\\n        error = True\\n    elif type_text[0] == \\'10-K/A\\':\\n        move_file(fh, fn, \"_amended\", tic, cik, filename, \"Amended 10-K - moved file\")\\n        error = True\\n    if error: return error\\n\\n    # Get the \\'acceptance-datetime\\' metadata element\\n    acc_dt = root.xpath(\\'//acceptance-datetime/text()\\')\\n    if acc_dt is None or len(acc_dt) == 0:\\n        header_text = None\\n        # If we didn\\'t find an <acceptance-datetime /> element, find the date elsewhere\\n        if len(root.xpath(\\'//sec-header/text()\\')) != 0:\\n            header_text = root.xpath(\\'//sec-header/text()\\')[0]\\n        elif len(root.xpath(\\'//ims-header/text()\\')) != 0:\\n            header_text = root.xpath(\\'//ims-header/text()\\')[0]\\n\\n        if header_text:\\n            filing_dt_text = re.sub(regex, r\"\\\\2\", header_text)\\n        else:\\n            move_file(fh, fn, \"_error\", tic, cik, filename, \"Bad filing date - moved file\")\\n            error = True\\n        if error: return error\\n    else:\\n        # Get the filing date\\n        filing_dt_text = acc_dt[0].split(\\'\\\\n\\', 1)[0][:8]\\n\\n    filing_dt = dt.strptime(filing_dt_text, \\'%Y%m%d\\')\\n    filing_ts = time.mktime(filing_dt.timetuple())\\n    begin_dt = dt(1995, 1, 1)\\n\\n    # If the filing date is not within our date range, then move it\\n    if begin_dt > filing_dt:\\n        move_file(fh, fn, \"_outofrange\", tic, cik, filename, \"Out of date range - moved file.\")\\n        error = True\\n    if error: return error\\n\\n    # See if we can find stock info for this company on the filing date of the 10-K\\n    cdef int index = 0\\n    cik_df = None\\n    try:\\n        index = df.index.get_loc((bytes(cik, \\'utf-8\\'), filing_dt))\\n        cik_df = df.ix[bytes(cik, \\'utf-8\\')]\\n        price = cik_df.ix[filing_dt, \\'PRC\\']\\n        # Now, check if the price of the stock is less than $3.00\\n        if price < 3.0:\\n            move_file(fh, fn, \"_nostockdata\", tic, cik, filename, \"Price less than $3.00 - moved file.\")\\n            error = True\\n    except (IndexError, KeyError):\\n        # We couldn\\'t find the cik or date for this 10-k\\n        move_file(fh, fn, \"_nostockdata\", tic, cik, filename, \"No stock data found - moved file.\")\\n        error = True\\n    if error: return error\\n    \\n    # Grab the report\\n    cdef str report = \\'\\'.join(root.xpath(\\'//document/text\\')[0].itertext())\\n\\n    # We will tokenize the text and iterate through each word\\n    cdef list tokens = report.split()\\n    cdef list keep_tokens = []\\n    cdef set stopwords_set = set(stopwords.words(\\'english\\'))\\n    punc_table = str.maketrans(\"\", \"\", string.punctuation)\\n    \\n    # Filter out words\\n    cdef str word\\n    for word in tokens:\\n        # Quick check to make sure we should keep filtering the word\\n        if len(word) != 1:\\n            # Strip punctuation from the word first and make it lowercase\\n            word = word.translate(punc_table).lower()\\n\\n            # Add the word to the keep pile if it is not a stopword and if it is in 2of12inf dictionary\\n            if word not in stopwords_set and word in dict_2of12inf:\\n                keep_tokens.append(word)\\n            \\n    tokens = keep_tokens\\n    report = \" \".join(tokens)\\n    cdef int total_words = len(tokens)\\n\\n    # Gather info for report to save into redis\\n    report_hash = {\\n        \\'cik\\': cik,\\n        \\'tic\\': tic,\\n        \\'path\\': fn,\\n        \\'file_name\\': filename,\\n        \\'filing_date\\': filing_ts,\\n        \\'year\\': filing_dt.year,\\n        \\'report\\': report,\\n        \\'total_words\\': total_words,\\n        \\'company_data\\': pickle.dumps(cik_df),\\n        \\'index\\': index,\\n        \\'mtime\\': time.time()\\n    }\\n\\n    # Close the file handle\\n    fh.close()\\n    \\n    # Save the stuff to redis\\n    print(\"Saving to redis: \" + report_key)\\n    rds.hmset(report_key, report_hash)\\n    rds.set(cleaned_key, time.time())'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2113\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-124>\u001b[0m in \u001b[0;36mcython\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/Cython/Build/IpythonMagic.py\u001b[0m in \u001b[0;36mcython\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_code_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_dynamic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_import_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/imp.py\u001b[0m in \u001b[0;36mload_dynamic\u001b[0;34m(name, path, file)\u001b[0m\n\u001b[1;32m    340\u001b[0m         spec = importlib.machinery.ModuleSpec(\n\u001b[1;32m    341\u001b[0m             name=name, loader=loader, origin=path)\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/importlib/_bootstrap.py\u001b[0m in \u001b[0;36mmodule_from_spec\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mcreate_module\u001b[0;34m(self, spec)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[0;32m_cython_magic_ca245bb9b7e072e3a3ea7089503aac0c.pyx\u001b[0m in \u001b[0;36minit _cython_magic_ca245bb9b7e072e3a3ea7089503aac0c (/usr/home/andrew/.cache/ipython/cython/_cython_magic_ca245bb9b7e072e3a3ea7089503aac0c.c:5099)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name df"
     ]
    }
   ],
   "source": [
    "%%cython\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "import pickle\n",
    "from datetime import datetime as dt\n",
    "from lxml import etree\n",
    "from io import StringIO\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "from __main__ import move_file, df, stopwords, rds, dict_2of12inf\n",
    "\n",
    "from cpython cimport bool\n",
    "\n",
    "regex = re.compile(r\"(.*\\.sgml\\s+?:\\s+?|.*\\nFILED AS OF DATE:\\s+?)([\\d]+?)\\n.*\", re.S)\n",
    "\n",
    "# This function handles the cleaning of the 10-K\n",
    "def clean(str fn):\n",
    "    cdef bool error = False\n",
    "    \n",
    "    cdef str s = os.sep\n",
    "    cdef str tic = fn.split(s)[1]\n",
    "    cdef str cik = fn.split(s)[2]\n",
    "    cdef str filename = fn.split(s)[4]\n",
    "    cdef str report_key = \"report:\" + cik + \":\" + fn\n",
    "    cdef str cleaned_key = \"cleaned:\" + cik + \":\" + fn\n",
    "\n",
    "    # Open the file, get all of the content, and then pull it into a parser\n",
    "    fh = open(fn, 'r')\n",
    "    cdef unicode contents = fh.read()\n",
    "\n",
    "    # Clean up some of the text to fix malformed HTML before parsing it\n",
    "    cdef list malformed_tags = ['ACCEPTANCE-DATETIME', 'TYPE', 'SEQUENCE', 'FILENAME', 'DESCRIPTION']\n",
    "    cdef str tag\n",
    "    for tag in malformed_tags:\n",
    "        # Do a regex that replaces all of these malformed tags in the document\n",
    "        regex = re.compile(r\"(\\n<%s>[^<]*?)\\n\" % re.escape(tag), re.I)\n",
    "        contents = regex.sub(r\"\\1</%s>\\n\" % tag, contents)\n",
    "\n",
    "    # Create the parser\n",
    "    parser = etree.HTMLParser()\n",
    "    document = etree.parse(StringIO(contents), parser)\n",
    "    doc = document.getroot()\n",
    "    \n",
    "    # The document can either have a root node of sec-document or ims-document\n",
    "    if doc.xpath('//sec-document') is not None:\n",
    "        root = doc.xpath('//sec-document[1]')[0]\n",
    "    elif doc.xpath('//ims-document') is not None: \n",
    "        root = doc.xpath('//ims-document[1]')[0]\n",
    "    elif doc.xpath('//document') is not None:\n",
    "        root = doc.xpath('//document[1]')[0]\n",
    "    elif doc.xpath('//error') is not None:\n",
    "        root = None\n",
    "    else:\n",
    "        root = None\n",
    "        \n",
    "    if root is None:\n",
    "        # Root node error \n",
    "        move_file(fh, fn, \"_error\", tic, cik, filename, \"No root or erroneous root node - moved file\")\n",
    "        error = True\n",
    "    if error: return error\n",
    "\n",
    "    # Check if this is an amended 10-K and throw it out if so\n",
    "    type_text = root.xpath('//type/text()')\n",
    "    if type_text is None or len(type_text) == 0:\n",
    "        move_file(fh, fn, \"_error\", tic, cik, filename, \"Error finding type - moved file\")\n",
    "        error = True\n",
    "    elif type_text[0] == '10-K/A':\n",
    "        move_file(fh, fn, \"_amended\", tic, cik, filename, \"Amended 10-K - moved file\")\n",
    "        error = True\n",
    "    if error: return error\n",
    "\n",
    "    # Get the 'acceptance-datetime' metadata element\n",
    "    acc_dt = root.xpath('//acceptance-datetime/text()')\n",
    "    if acc_dt is None or len(acc_dt) == 0:\n",
    "        header_text = None\n",
    "        # If we didn't find an <acceptance-datetime /> element, find the date elsewhere\n",
    "        if len(root.xpath('//sec-header/text()')) != 0:\n",
    "            header_text = root.xpath('//sec-header/text()')[0]\n",
    "        elif len(root.xpath('//ims-header/text()')) != 0:\n",
    "            header_text = root.xpath('//ims-header/text()')[0]\n",
    "\n",
    "        if header_text:\n",
    "            filing_dt_text = re.sub(regex, r\"\\2\", header_text)\n",
    "        else:\n",
    "            move_file(fh, fn, \"_error\", tic, cik, filename, \"Bad filing date - moved file\")\n",
    "            error = True\n",
    "        if error: return error\n",
    "    else:\n",
    "        # Get the filing date\n",
    "        filing_dt_text = acc_dt[0].split('\\n', 1)[0][:8]\n",
    "\n",
    "    filing_dt = dt.strptime(filing_dt_text, '%Y%m%d')\n",
    "    filing_ts = time.mktime(filing_dt.timetuple())\n",
    "    begin_dt = dt(1995, 1, 1)\n",
    "\n",
    "    # If the filing date is not within our date range, then move it\n",
    "    if begin_dt > filing_dt:\n",
    "        move_file(fh, fn, \"_outofrange\", tic, cik, filename, \"Out of date range - moved file.\")\n",
    "        error = True\n",
    "    if error: return error\n",
    "\n",
    "    # See if we can find stock info for this company on the filing date of the 10-K\n",
    "    cdef int index = 0\n",
    "    cik_df = None\n",
    "    try:\n",
    "        index = df.index.get_loc((bytes(cik, 'utf-8'), filing_dt))\n",
    "        cik_df = df.ix[bytes(cik, 'utf-8')]\n",
    "        price = cik_df.ix[filing_dt, 'PRC']\n",
    "        # Now, check if the price of the stock is less than $3.00\n",
    "        if price < 3.0:\n",
    "            move_file(fh, fn, \"_nostockdata\", tic, cik, filename, \"Price less than $3.00 - moved file.\")\n",
    "            error = True\n",
    "    except (IndexError, KeyError):\n",
    "        # We couldn't find the cik or date for this 10-k\n",
    "        move_file(fh, fn, \"_nostockdata\", tic, cik, filename, \"No stock data found - moved file.\")\n",
    "        error = True\n",
    "    if error: return error\n",
    "    \n",
    "    # Grab the report\n",
    "    cdef str report = ''.join(root.xpath('//document/text')[0].itertext())\n",
    "\n",
    "    # We will tokenize the text and iterate through each word\n",
    "    cdef list tokens = report.split()\n",
    "    cdef list keep_tokens = []\n",
    "    cdef set stopwords_set = set(stopwords.words('english'))\n",
    "    punc_table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    \n",
    "    # Filter out words\n",
    "    cdef str word\n",
    "    for word in tokens:\n",
    "        # Quick check to make sure we should keep filtering the word\n",
    "        if len(word) != 1:\n",
    "            # Strip punctuation from the word first and make it lowercase\n",
    "            word = word.translate(punc_table).lower()\n",
    "\n",
    "            # Add the word to the keep pile if it is not a stopword and if it is in 2of12inf dictionary\n",
    "            if word not in stopwords_set and word in dict_2of12inf:\n",
    "                keep_tokens.append(word)\n",
    "            \n",
    "    tokens = keep_tokens\n",
    "    report = \" \".join(tokens)\n",
    "    cdef int total_words = len(tokens)\n",
    "\n",
    "    # Gather info for report to save into redis\n",
    "    report_hash = {\n",
    "        'cik': cik,\n",
    "        'tic': tic,\n",
    "        'path': fn,\n",
    "        'file_name': filename,\n",
    "        'filing_date': filing_ts,\n",
    "        'year': filing_dt.year,\n",
    "        'report': report,\n",
    "        'total_words': total_words,\n",
    "        'company_data': pickle.dumps(cik_df),\n",
    "        'index': index,\n",
    "        'mtime': time.time()\n",
    "    }\n",
    "\n",
    "    # Close the file handle\n",
    "    fh.close()\n",
    "    \n",
    "    # Save the stuff to redis\n",
    "    print(\"Saving to redis: \" + report_key)\n",
    "    rds.hmset(report_key, report_hash)\n",
    "    rds.set(cleaned_key, time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "import os\n",
    "import pickle\n",
    "import math\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from __main__ import df, rds, pos_dict, pos_roots, pos_roots_map, neg_dict, neg_roots, neg_roots_map\n",
    "\n",
    "from cpython cimport bool\n",
    "\n",
    "def process(str fn):\n",
    "    \n",
    "    cdef str s = os.sep\n",
    "    cdef str tic = fn.split(s)[1]\n",
    "    cdef str cik = fn.split(s)[2]\n",
    "    cdef str report_key = \"report:\" + cik + \":\" + fn\n",
    "    cdef str processed_key = \"processed:\"+ cik + \":\" + fn\n",
    "    \n",
    "    # Get the report out of redis\n",
    "    #print(\"Found in redis: \" + report_key)\n",
    "    cdef str report = str(rds.hget(report_key, 'report'))\n",
    "    filing_dt = dt.fromtimestamp(int(float(rds.hget(report_key, 'filing_date').decode('utf-8'))))\n",
    "    cik_df = pickle.loads(rds.hget(report_key, 'company_data'))\n",
    "    cdef int index = int(rds.hget(report_key, 'index'))\n",
    "    cdef dict report_hash = {}\n",
    "\n",
    "    # Now that everything is cleaned up, we can run the word processing algorithm\n",
    "    pos_occurs = defaultdict(int)\n",
    "    neg_occurs = defaultdict(int)\n",
    "    negators = pd.Series(['not', 'no', 'never'])\n",
    "\n",
    "    # We will tokenize the text and iterate through each word\n",
    "    tokens = pd.Series(report.split())\n",
    "\n",
    "    # Now, process the text\n",
    "    cdef int i\n",
    "    cdef str token, root, word\n",
    "    cdef bool negated\n",
    "    for i, token in tokens.iteritems():\n",
    "        if token in pos_dict:\n",
    "            # Check to see if there is a negator\n",
    "            negated = False\n",
    "            for word in tokens.iloc[(i - 3):(i + 3)]:\n",
    "                if word in negators.values:\n",
    "                    #print(\"Found a negator: \" + word + \" - \" + token)\n",
    "                    negated = True\n",
    "            if not negated:\n",
    "                root = pos_roots_map[token]\n",
    "                pos_occurs[root] += 1\n",
    "        elif token in neg_dict:\n",
    "            # Check to see if there is a negator\n",
    "            negated = False\n",
    "            for word in tokens.iloc[(i - 3):(i + 3)]:\n",
    "                if word in negators.values:\n",
    "                    #print(\"Found a negator: \" + word + \" - \" + token)\n",
    "                    negated = True\n",
    "            if not negated:\n",
    "                root = neg_roots_map[token]\n",
    "                neg_occurs[root] += 1\n",
    "\n",
    "    # For the roots we didn't find, set frequency to zero\n",
    "    for root in pos_roots:\n",
    "        if root not in pos_occurs:\n",
    "            pos_occurs[root] = 0\n",
    "    for root in neg_roots:\n",
    "        if root not in neg_occurs:\n",
    "            neg_occurs[root] = 0\n",
    "            \n",
    "    # Use the index we found earlier to grab the historical info\n",
    "    hist_returns = cik_df.ix[(index + 1):, 'RET']\n",
    "\n",
    "    # Calculate the historical return before the filing date\n",
    "    cdef float hist_ret = 1.0\n",
    "    for col, series in hist_returns.iteritems():\n",
    "        if col == 'RET':\n",
    "            for r in series:\n",
    "                if not math.isnan(r):\n",
    "                    hist_ret *= (r + 1.0)\n",
    "    hist_ret = hist_ret - 1.0\n",
    "    #print(\"Historical return: \" + str(hist_ret))\n",
    "\n",
    "    # Use the index we found earlier to grab the four day window returns\n",
    "    returns = cik_df.ix[(index - 3):(index + 1), ['RET','vwretd']]\n",
    "\n",
    "    # Calculate the abnormal return: r_i = M{t=0, 3} (ret_i,j) - M{t=0,3} (ret_vwi,t)\n",
    "    cdef float ret = 1.0\n",
    "    cdef float ret_vwi = 1.0\n",
    "    for col, series in returns.iteritems():\n",
    "        if col == 'RET':\n",
    "            for r in series:\n",
    "                if not math.isnan(r):\n",
    "                    ret *= (r + 1.0)\n",
    "        elif col == 'vwretd':\n",
    "            for r in series:\n",
    "                if  not math.isnan(r):\n",
    "                    ret_vwi *= (r + 1.0)\n",
    "    cdef float ab_ret = ((ret - 1.0) - (ret_vwi - 1.0))\n",
    "    #print(\"Abnormal return: \" + str(ab_ret))\n",
    "\n",
    "    # Save results of text processing to key in redis\n",
    "    report_hash['pos_occurs'] = pickle.dumps(pos_occurs)\n",
    "    report_hash['neg_occurs'] = pickle.dumps(neg_occurs)\n",
    "    report_hash['hist_ret'] = hist_ret\n",
    "    report_hash['ab_ret'] = ab_ret\n",
    "    report_hash['mtime'] = time.time()\n",
    "\n",
    "    print(\"Saving to redis: \" + report_key)\n",
    "    rds.hmset(report_key, report_hash)\n",
    "    rds.set(processed_key, time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "import os\n",
    "\n",
    "from __main__ import check_redis, clean, process\n",
    "\n",
    "from cpython cimport bool\n",
    "\n",
    "# This is for testing\n",
    "cdef int count = 1\n",
    "cdef int stop = 100000\n",
    "cdef bool skip_cleaned = True\n",
    "cdef bool skip_processed = True\n",
    "cdef bool process_file = True\n",
    "\n",
    "cdef dict report_hash\n",
    "cdef str fn, s, tic, cik, cleaned_key, processed_key, report_key\n",
    "cdef bool cleaned, processed, error\n",
    "\n",
    "cdef str dirpath\n",
    "cdef list dirnames, filenames\n",
    "cdef str folder = \"SEC-Edgar-data\"\n",
    "for (dirpath, dirnames, filenames) in os.walk(folder, topdown=False):\n",
    "    for filename in filenames:\n",
    "        report_hash = {}\n",
    "        fn = os.sep.join([dirpath, filename])\n",
    "        \n",
    "        if filename.endswith('.txt'):# and filename == \"0000950116-97-000637.txt\":\n",
    "            if count > stop:\n",
    "                break\n",
    "            print(fn)\n",
    "            s = os.sep\n",
    "            tic = fn.split(s)[1]\n",
    "            cik = fn.split(s)[2]\n",
    "            \n",
    "            # Check redis to see if we have processed or cleaned the report already\n",
    "            cleaned_key = \"cleaned:\" + cik + \":\" + fn\n",
    "            processed_key = \"processed:\" + cik + \":\" + fn\n",
    "            report_key = \"report:\" + cik + \":\" + fn\n",
    "            (cleaned, processed) = check_redis(cleaned_key, processed_key, report_key)\n",
    "            \n",
    "            # If the report has been cleaned or we don't want to clean it anyway, skip this step\n",
    "            error = False\n",
    "            if not cleaned or not skip_cleaned:\n",
    "                print(\"(\" + str(count) + \") Cleaning \" + fn)\n",
    "                error = clean(fn)\n",
    "                \n",
    "                if not process and not error:\n",
    "                    count += 1\n",
    "                    continue\n",
    "            if error: continue\n",
    "            \n",
    "            # After possibly cleaning, check if we should process the file\n",
    "            if (not processed or not skip_processed) and process_file:\n",
    "                print(\"(\" + str(count) + \") Processing \" + fn)\n",
    "                process(fn)\n",
    "                \n",
    "                count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found yearly data in Redis.\n",
      "1996 2502\n",
      "1997 3652\n",
      "1998 3994\n",
      "1999 3792\n",
      "2000 3882\n",
      "2001 3208\n",
      "2002 2619\n",
      "2003 2684\n",
      "2004 3038\n",
      "2005 2913\n",
      "2006 2949\n",
      "2007 2986\n",
      "2008 2727\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "stop = math.inf\n",
    "yearly_data = {}\n",
    "\n",
    "if rds.exists(\"yearly-data\"):\n",
    "    print(\"Found yearly data in Redis.\")\n",
    "    yearly_data = pickle.loads(zlib.decompress(rds.get(\"yearly-data\")))\n",
    "else:\n",
    "    keys = rds.keys(\"report:*\")\n",
    "    errors = []\n",
    "    for key in keys:\n",
    "\n",
    "        if count >= stop:\n",
    "            break\n",
    "\n",
    "        report_hash = rds.hgetall(key)\n",
    "        try:\n",
    "            year = 1\n",
    "            cik = str(report_hash[b'cik'].decode('utf-8'))\n",
    "            fn = str(report_hash[b'path'].decode('utf-8'))\n",
    "            key = \"report:\" + cik + \":\" + fn\n",
    "            pos_occurs = pickle.loads(report_hash[b'pos_occurs'])\n",
    "            neg_occurs = pickle.loads(report_hash[b'neg_occurs'])\n",
    "            year = int(report_hash[b'year'])\n",
    "            total_words = int(report_hash[b'total_words'])\n",
    "            hist_ret = float(report_hash[b'hist_ret'])\n",
    "            ab_ret = float(report_hash[b'ab_ret'])\n",
    "\n",
    "            if total_words == 0:\n",
    "                # Not sure why this would be zero, but we may need to reprocess\n",
    "                print(\"Error with: \" + key)\n",
    "                cleaned_key = \"cleaned:\" + cik + \":\" + fn\n",
    "                processed_key = \"processed:\" + cik + \":\" + fn\n",
    "\n",
    "                # Delete this error from redis\n",
    "                rds.delete(cleaned_key, processed_key, key)\n",
    "\n",
    "                errors.append(key)\n",
    "                continue\n",
    "\n",
    "            try: yearly_data[year]\n",
    "            except KeyError:\n",
    "                yearly_data[year] = []\n",
    "\n",
    "            year_list = yearly_data[year]\n",
    "            year_list.append({\n",
    "                'pos_occurs': pos_occurs,\n",
    "                'neg_occurs': neg_occurs,\n",
    "                'total_words': total_words,\n",
    "                'hist_ret': hist_ret,\n",
    "                'ab_ret': ab_ret\n",
    "            })\n",
    "            yearly_data[year] = year_list\n",
    "\n",
    "            count += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "        except e:\n",
    "            print(e)\n",
    "            \n",
    "    print(\"Saving yearly data in Redis.\")\n",
    "    rds.set(\"yearly-data\", zlib.compress(pickle.dumps(yearly_data)))\n",
    "    \n",
    "    if len(errors) > 0:\n",
    "        print(\"Total errors: \" + str(len(errors)))\n",
    "        \n",
    "for year in sorted(yearly_data.keys()):\n",
    "    print(year, len(yearly_data[year]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rds.exists(\"regression:\" + str(1997))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from __main__ import rds, yearly_data\n",
    "\n",
    "from cpython cimport bool\n",
    "\n",
    "cdef int start = 1997\n",
    "cdef int end = 2008\n",
    "cdef bool skip_regression = True\n",
    "cdef bool skip_building = True\n",
    "cdef int t\n",
    "cdef int year\n",
    "cdef int count\n",
    "cdef int total\n",
    "cdef str key\n",
    "\n",
    "# Clear some variables so we have enough RAM...\n",
    "#try: del data, df\n",
    "#except NameError:\n",
    "#    pass\n",
    "\n",
    "# Generate a rolling training model using data up until year T-1\n",
    "for t in range(start, (end + 1)):\n",
    "    \n",
    "    print(\"Analyzing year \" + str(t))\n",
    "    \n",
    "    # Skip this analysis if redis already has it and the boolean is set to skip\n",
    "    if rds.exists(\"regression:\" + str(t)) and skip_regression:\n",
    "        continue\n",
    "\n",
    "    pos_word_weights = pd.DataFrame()\n",
    "    neg_word_weights = pd.DataFrame()\n",
    "    hist_returns = pd.DataFrame()\n",
    "    ab_returns = pd.DataFrame()\n",
    "    \n",
    "    # Iterate over each year before year T and build the training data set\n",
    "    for year in range((start - 1), t):\n",
    "        \n",
    "        key = \"regression-data:\" + str(year)\n",
    "        \n",
    "        if rds.exists(key) and skip_building:\n",
    "            # Extract the data\n",
    "            pos_word_weights = pickle.loads(rds.hget(key, 'pos_word_weights'))\n",
    "            neg_word_weights = pickle.loads(rds.hget(key, 'neg_word_weights'))\n",
    "            hist_returns = pickle.loads(rds.hget(key, 'hist_returns'))\n",
    "            ab_returns = pickle.loads(rds.hget(key, 'ab_returns'))\n",
    "            continue\n",
    "        \n",
    "        print(\"Building year \" + str(year) + \"/\" + str(t))\n",
    "        \n",
    "        try: yearly_data[year]\n",
    "        except KeyError:\n",
    "            print(\"Year \" + str(year) + \" not found.\")\n",
    "            continue\n",
    "        \n",
    "        # Iterate through each 10-K info for the year and generate the dataframe for the regression\n",
    "        count = 0\n",
    "        total = len(yearly_data[year])\n",
    "        for report in yearly_data[year]:\n",
    "            count += 1\n",
    "            print(\"Report \" + str(count) + \"/\" + str(total))\n",
    "            \n",
    "            a = report['total_words']\n",
    "            hist_ret = report['hist_ret']\n",
    "            ab_ret = report['ab_ret']\n",
    "            \n",
    "            weights = {}\n",
    "            pos_occurs = report['pos_occurs']\n",
    "            for word in pos_occurs.keys():\n",
    "                F = pos_occurs[word]\n",
    "                weights[word] = F/(a * 1.0)\n",
    "            pos_word_weights = pos_word_weights.append(weights, ignore_index=True)\n",
    "            \n",
    "            weights = {}\n",
    "            neg_occurs = report['neg_occurs']\n",
    "            for word in neg_occurs.keys():\n",
    "                F = neg_occurs[word]\n",
    "                weights[word] = F/(a * 1.0)\n",
    "            neg_word_weights = neg_word_weights.append(weights, ignore_index=True)\n",
    "            \n",
    "            hist_returns = hist_returns.append({'hist_ret': hist_ret}, ignore_index=True)\n",
    "            ab_returns = ab_returns.append({'ab_ret': ab_ret}, ignore_index=True)\n",
    "            \n",
    "        # Save our progress so we don't have to recalculate this year again\n",
    "        # This will cont\n",
    "        print(\"Saving progress for year \" + str(year) + \" to redis.\")\n",
    "        reg_data_hash = {\n",
    "            'pos_word_weights': pickle.dumps(pos_word_weights),\n",
    "            'neg_word_weights': pickle.dumps(neg_word_weights),\n",
    "            'hist_returns': pickle.dumps(hist_returns),\n",
    "            'ab_returns': pickle.dumps(ab_returns)\n",
    "        }\n",
    "        rds.hmset(key, reg_data_hash)\n",
    "        \n",
    "    # Run the regressions for all years up to year T\n",
    "    if not ab_returns.empty and not hist_returns.empty and not pos_word_weights.empty and not neg_word_weights.empty:\n",
    "        hist_returns.reset_index()\n",
    "        hist_returns_series = pd.Series(hist_returns['hist_ret'])\n",
    "        ab_returns.reset_index()\n",
    "        ab_returns_series = pd.Series(ab_returns['ab_ret'])\n",
    "        pos_word_weights.reset_index()\n",
    "        neg_word_weights.reset_index()\n",
    "        \n",
    "        # Estimate the weights for the words using a regression\n",
    "        print(\"T = \" + str(t) + \": Estimating weights\")\n",
    "        pos_reg = sm.OLS(hist_returns_series, pos_word_weights)\n",
    "        pos_model = pos_reg.fit()\n",
    "        neg_reg = sm.OLS(hist_returns_series, neg_word_weights)\n",
    "        neg_model = neg_reg.fit()\n",
    "        \n",
    "        # Map the words to their coefficients\n",
    "        pos_coeffs_dict = dict(zip(list(pos_word_weights.columns), pos_model.params))\n",
    "        pos_coeffs = pd.DataFrame(list(pos_coeffs_dict.items()), columns=['word','weight'])\n",
    "        neg_coeffs_dict = dict(zip(list(neg_word_weights.columns), neg_model.params))\n",
    "        neg_coeffs = pd.DataFrame(list(neg_coeffs_dict.items()), columns=['word','weight'])\n",
    "    \n",
    "        # Calculate the average word weight as well as the standard deviation\n",
    "        pos_avg = pos_coeffs['weight'].mean()\n",
    "        pos_std = pos_coeffs['weight'].std()\n",
    "        neg_avg = neg_coeffs['weight'].mean()\n",
    "        neg_std = neg_coeffs['weight'].std()\n",
    "        #print(\"Average: \" + str(pos_avg) + \"; StdDev: \" + str(pos_std))\n",
    "        #print(\"Average: \" + str(neg_avg) + \"; StdDev: \" + str(neg_std))\n",
    "        #print(pos_coeffs)\n",
    "\n",
    "        # Normalize the weights of the words\n",
    "        print(\"T = \" + str(t) + \": Normalizing weights\")\n",
    "        pos_norm = list()\n",
    "        for col, series in pos_coeffs.iteritems():\n",
    "            if col == 'weight':\n",
    "                for weight in series:\n",
    "                    pos_norm.append((weight - pos_avg) / pos_std)\n",
    "        pos_coeffs['norm_weight'] = pd.Series(pos_norm, index=pos_coeffs.index)\n",
    "        \n",
    "        neg_norm = list()\n",
    "        for col, series in neg_coeffs.iteritems():\n",
    "            if col == 'weight':\n",
    "                for weight in series:\n",
    "                    neg_norm.append((weight - neg_avg) / neg_std)\n",
    "        neg_coeffs['norm_weight'] = pd.Series(neg_norm, index=neg_coeffs.index)\n",
    "        \n",
    "        # Iterate through the original word weights and apply the normalized weight\n",
    "        for word, series in pos_word_weights.iteritems():\n",
    "            norm_weight = pos_coeffs.loc[pos_coeffs['word'] == word]['norm_weight']\n",
    "            pos_word_weights[word] = series.apply(lambda x: x * norm_weight)\n",
    "        for word, series in neg_word_weights.iteritems():\n",
    "            norm_weight = neg_coeffs.loc[neg_coeffs['word'] == word]['norm_weight']\n",
    "            neg_word_weights[word] = series.apply(lambda x: x * norm_weight)\n",
    "                \n",
    "        # Run the regression for abnormal (after filing) returns using the estimated weights for the words\n",
    "        print(\"T = \" + str(t) + \": Doing regression\")\n",
    "        pos_ab_reg = sm.OLS(ab_returns_series, pos_word_weights)\n",
    "        pos_ab_model = pos_ab_reg.fit()\n",
    "        neg_ab_reg = sm.OLS(ab_returns_series, neg_word_weights)\n",
    "        neg_ab_model = neg_ab_reg.fit()\n",
    "        \n",
    "        # Map the words to their coefficients\n",
    "        pos_coeffs_dict = dict(zip(list(pos_word_weights.columns), pos_ab_model.params))\n",
    "        pos_coeffs = pd.DataFrame(list(pos_coeffs_dict.items()), columns=['word','weight'])\n",
    "        neg_coeffs_dict = dict(zip(list(neg_word_weights.columns), neg_ab_model.params))\n",
    "        neg_coeffs = pd.DataFrame(list(neg_coeffs_dict.items()), columns=['word','weight'])\n",
    "\n",
    "        # Calculate the score of each document using the weights for each word given by the regression\n",
    "        # TODO: we should iterate through each report and insert the calculated score for each document right?\n",
    "        #score = math.nan\n",
    "        \n",
    "        key = \"regression:\" + str(t)\n",
    "        \n",
    "        reg_hash = {\n",
    "            'pos_model': pickle.dumps(pos_model),\n",
    "            'neg_model': pickle.dumps(neg_model),\n",
    "            'pos_ab_model': pickle.dumps(pos_ab_model),\n",
    "            'neg_ab_model': pickle.dumps(neg_ab_model)\n",
    "        }\n",
    "        \n",
    "        rds.hmset(key, reg_hash)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Weight Quintiles by Term Frequency Quintiles\n",
      "\n",
      "Weight Quintile 1: Total words = 25\n",
      "Term Freq Quintile 1: 36.0\n",
      "Term Freq Quintile 2: 24.0\n",
      "Term Freq Quintile 3: 20.0\n",
      "Term Freq Quintile 4: 4.0\n",
      "Term Freq Quintile 5: 16.0\n",
      "\n",
      "Weight Quintile 2: Total words = 24\n",
      "Term Freq Quintile 1: 12.5\n",
      "Term Freq Quintile 2: 29.166666666666668\n",
      "Term Freq Quintile 3: 12.5\n",
      "Term Freq Quintile 4: 33.33333333333333\n",
      "Term Freq Quintile 5: 12.5\n",
      "\n",
      "Weight Quintile 3: Total words = 25\n",
      "Term Freq Quintile 1: 16.0\n",
      "Term Freq Quintile 2: 12.0\n",
      "Term Freq Quintile 3: 28.000000000000004\n",
      "Term Freq Quintile 4: 12.0\n",
      "Term Freq Quintile 5: 32.0\n",
      "\n",
      "Weight Quintile 4: Total words = 24\n",
      "Term Freq Quintile 1: 25.0\n",
      "Term Freq Quintile 2: 20.833333333333336\n",
      "Term Freq Quintile 3: 12.5\n",
      "Term Freq Quintile 4: 33.33333333333333\n",
      "Term Freq Quintile 5: 8.333333333333332\n",
      "\n",
      "Weight Quintile 5: Total words = 25\n",
      "Term Freq Quintile 1: 12.0\n",
      "Term Freq Quintile 2: 12.0\n",
      "Term Freq Quintile 3: 28.000000000000004\n",
      "Term Freq Quintile 4: 16.0\n",
      "Term Freq Quintile 5: 32.0\n",
      "\n",
      "Negative Weight Quintiles by Term Frequency Quintiles\n",
      "\n",
      "Weight Quintile 1: Total words = 144\n",
      "Term Freq Quintile 1: 20.13888888888889\n",
      "Term Freq Quintile 2: 20.833333333333336\n",
      "Term Freq Quintile 3: 22.916666666666664\n",
      "Term Freq Quintile 4: 20.13888888888889\n",
      "Term Freq Quintile 5: 15.972222222222221\n",
      "\n",
      "Weight Quintile 2: Total words = 143\n",
      "Term Freq Quintile 1: 15.384615384615385\n",
      "Term Freq Quintile 2: 20.27972027972028\n",
      "Term Freq Quintile 3: 20.97902097902098\n",
      "Term Freq Quintile 4: 25.174825174825177\n",
      "Term Freq Quintile 5: 18.181818181818183\n",
      "\n",
      "Weight Quintile 3: Total words = 144\n",
      "Term Freq Quintile 1: 25.694444444444443\n",
      "Term Freq Quintile 2: 19.444444444444446\n",
      "Term Freq Quintile 3: 18.75\n",
      "Term Freq Quintile 4: 9.722222222222223\n",
      "Term Freq Quintile 5: 26.38888888888889\n",
      "\n",
      "Weight Quintile 4: Total words = 143\n",
      "Term Freq Quintile 1: 13.986013986013987\n",
      "Term Freq Quintile 2: 20.97902097902098\n",
      "Term Freq Quintile 3: 17.482517482517483\n",
      "Term Freq Quintile 4: 24.475524475524477\n",
      "Term Freq Quintile 5: 23.076923076923077\n",
      "\n",
      "Weight Quintile 5: Total words = 144\n",
      "Term Freq Quintile 1: 25.0\n",
      "Term Freq Quintile 2: 18.055555555555554\n",
      "Term Freq Quintile 3: 20.13888888888889\n",
      "Term Freq Quintile 4: 20.13888888888889\n",
      "Term Freq Quintile 5: 16.666666666666664\n",
      "Top Five Most Positive and Negative Words within Frequency Quintiles\n",
      "====================================================================\n",
      "\n",
      "Top 5 Most Positive Words\n",
      "==========================\n",
      "\n",
      "Bucket 1\n",
      "lucrative\n",
      "fantastic\n",
      "unparalleled\n",
      "destined\n",
      "impress\n",
      "\n",
      "Bucket 2\n",
      "confident\n",
      "exemplary\n",
      "pleasant\n",
      "surpass\n",
      "preeminence\n",
      "\n",
      "Bucket 3\n",
      "honor\n",
      "solves\n",
      "diligent\n",
      "ideal\n",
      "versatile\n",
      "\n",
      "Bucket 4\n",
      "desirable\n",
      "adequately\n",
      "positive\n",
      "resolve\n",
      "stability\n",
      "\n",
      "Bucket 5\n",
      "exclusive\n",
      "good\n",
      "leading\n",
      "great\n",
      "able\n",
      "\n",
      "Top 5 Most Negative Words\n",
      "==========================\n",
      "\n",
      "Bucket 1\n",
      "unfeasible\n",
      "vitiation\n",
      "absenteeism\n",
      "derogatory\n",
      "ridiculing\n",
      "\n",
      "Bucket 2\n",
      "unoccupied\n",
      "bribing\n",
      "egregiously\n",
      "laundering\n",
      "overproduction\n",
      "\n",
      "Bucket 3\n",
      "controversy\n",
      "inconvenient\n",
      "maliciously\n",
      "depriving\n",
      "urgent\n",
      "\n",
      "Bucket 4\n",
      "shutting\n",
      "disqualifying\n",
      "disturbs\n",
      "misusing\n",
      "intentional\n",
      "\n",
      "Bucket 5\n",
      "deteriorations\n",
      "closures\n",
      "prosecutions\n",
      "plaintiffs\n",
      "challenging\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:119: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:130: PerformanceWarning: indexing past lexsort depth may impact performance.\n"
     ]
    }
   ],
   "source": [
    "# This will store the global positive and negative words occurrances\n",
    "pos_occurs_all = defaultdict(int)\n",
    "neg_occurs_all = defaultdict(int)\n",
    "\n",
    "try: yearly_data\n",
    "except NameError: yearly_data = pickle.loads(zlib.decompress(rds.get('yearly-data')))\n",
    "\n",
    "if rds.exists(\"freq:pos:all\") and rds.exists(\"freq:neg:all\"):\n",
    "    pos_occurs_all = pickle.loads(zlib.decompress(rds.get(\"freq:pos:all\")))\n",
    "    neg_occurs_all = pickle.loads(zlib.decompress(rds.get(\"freq:neg:all\")))\n",
    "else:\n",
    "    for year in yearly_data:\n",
    "        print(\"Counting frequency for year \" + str(year))\n",
    "\n",
    "        pos_occurs_year = defaultdict(int)\n",
    "        neg_occurs_year = defaultdict(int)\n",
    "        pos_key = \"freq:pos:\" + str(year)\n",
    "        neg_key = \"freq:neg:\" + str(year)\n",
    "\n",
    "        if rds.exists(pos_key) and rds.exists(neg_key):\n",
    "            pos_occurs_year = pickle.loads(zlib.decompress(rds.get(pos_key)))\n",
    "            neg_occurs_year = pickle.loads(zlib.decompress(rds.get(neg_key)))\n",
    "        else:\n",
    "            count = 0\n",
    "            total = len(yearly_data[year])\n",
    "            for report in yearly_data[year]:\n",
    "                count += 1\n",
    "                print(\"Counting Report \" + str(count) + \"/\" + str(total))\n",
    "                report_pos_occurs = report['pos_occurs']\n",
    "                report_neg_occurs = report['neg_occurs']\n",
    "\n",
    "                for word, freq in report_pos_occurs.items():\n",
    "                    pos_occurs_year[word] += freq\n",
    "                for word, freq in report_neg_occurs.items():\n",
    "                    neg_occurs_year[word] += freq\n",
    "\n",
    "            print(\"Saving progress to Redis.\")\n",
    "            rds.set(\"freq:pos:\" + str(year), zlib.compress(pickle.dumps(pos_occurs_year)))\n",
    "            rds.set(\"freq:neg:\" + str(year), zlib.compress(pickle.dumps(neg_occurs_year)))\n",
    "\n",
    "    for word, freq in pos_occurs_year.items():\n",
    "        pos_occurs_all[word] += freq\n",
    "    for word, freq in neg_occurs_year.items():\n",
    "        neg_occurs_all[word] += freq\n",
    "        \n",
    "    print(\"Finished totaling and now saving to Redis.\")\n",
    "    rds.set(\"freq:pos:all\", zlib.compress(pickle.dumps(pos_occurs_all)))\n",
    "    rds.set(\"freq:neg:all\", zlib.compress(pickle.dumps(neg_occurs_all)))\n",
    "\n",
    "# Sort the values by highest to lowest\n",
    "pos_sorted = pd.Series(data=pos_occurs_all).sort_values()\n",
    "neg_sorted = pd.Series(data=neg_occurs_all).sort_values()\n",
    "\n",
    "# Get the coefficients\n",
    "pos_coeffs = pickle.loads(zlib.decompress(rds.hget(\"regression:2008\", 'pos_coeffs')))\n",
    "neg_coeffs = pickle.loads(zlib.decompress(rds.hget(\"regression:2008\", 'neg_coeffs')))\n",
    "\n",
    "# Place the weights into quintiles\n",
    "pos_wt_buckets = pd.qcut(pos_coeffs['weight'], 5, labels=range(1,6))\n",
    "neg_wt_buckets = pd.qcut(neg_coeffs['weight'], 5, labels=range(1,6))\n",
    "\n",
    "# Sort the coefficients by value\n",
    "pos_coeffs.sort_values('weight', inplace=True)\n",
    "neg_coeffs.sort_values('weight', inplace=True)\n",
    "\n",
    "# See if we have the total 10-Ks and number of appearances of each word in a 10-k\n",
    "try: total, pos_appearances, neg_appearances\n",
    "except NameError:\n",
    "    (total, pos_appearances, neg_appearances) = get_appearances()\n",
    "\n",
    "# Place the frequencies into quintiles\n",
    "pos_freq_buckets = pd.qcut(pos_appearances['apps'], 5, labels=range(1,6))\n",
    "neg_freq_buckets = pd.qcut(neg_appearances['apps'], 5, labels=range(1,6))\n",
    "\n",
    "bucket_freq_list = []\n",
    "bucket_wt_list = []\n",
    "freq_list = []\n",
    "total_list = []\n",
    "for (word, row) in pos_coeffs.iterrows():\n",
    "    bucket_freq_list.append(pos_freq_buckets.ix[word])\n",
    "    bucket_wt_list.append(pos_wt_buckets.ix[word])\n",
    "    freq_list.append(pos_sorted.ix[word])\n",
    "    total_list.append(pos_appearances.ix[word]['apps'])\n",
    "pos_coeffs['bucket_freq'] = pd.Series(bucket_freq_list, index=pos_coeffs.index)\n",
    "pos_coeffs['bucket_weight'] = pd.Series(bucket_wt_list, index=pos_coeffs.index)\n",
    "pos_coeffs['freq'] = pd.Series(freq_list, index=pos_coeffs.index)\n",
    "pos_coeffs['apps'] = pd.Series(total_list, index=pos_coeffs.index)\n",
    "pos_coeffs['files'] = pos_appearances['files']\n",
    "\n",
    "bucket_freq_list = []\n",
    "bucket_wt_list = []\n",
    "freq_list = []\n",
    "total_list = []\n",
    "for (word, row) in neg_coeffs.iterrows():\n",
    "    bucket_freq_list.append(neg_freq_buckets.ix[word])\n",
    "    bucket_wt_list.append(neg_wt_buckets.ix[word])\n",
    "    freq_list.append(neg_sorted.ix[word])\n",
    "    total_list.append(neg_appearances.ix[word]['apps'])\n",
    "neg_coeffs['bucket_freq'] = pd.Series(bucket_freq_list, index=neg_coeffs.index)\n",
    "neg_coeffs['bucket_weight'] = pd.Series(bucket_wt_list, index=neg_coeffs.index)\n",
    "neg_coeffs['freq'] = pd.Series(freq_list, index=neg_coeffs.index)\n",
    "neg_coeffs['apps'] = pd.Series(total_list, index=neg_coeffs.index)\n",
    "neg_coeffs['files'] = neg_appearances['files']\n",
    "\n",
    "# Build the table of weight quintiles to frequency quintiles\n",
    "pos_tab5 = pos_coeffs.reset_index(level='word').set_index(['bucket_weight','bucket_freq','word'])\n",
    "neg_tab5 = neg_coeffs.reset_index(level='word').set_index(['bucket_weight','bucket_freq','word'])\n",
    "\n",
    "# Iterate first through the weight buckets\n",
    "print(\"Positive Weight Quintiles by Term Frequency Quintiles\")\n",
    "for i in range(1,6):\n",
    "    # Get the total number of words in this weight quantile\n",
    "    tot_in_quantile = len(pos_tab5.ix[i]['weight'])\n",
    "    \n",
    "    print(\"\\nWeight Quintile \" + str(i) + \": Total words = \" + str(tot_in_quantile))\n",
    "    \n",
    "    # Find the number of words in each freq bucket and show that as percentage of words in weight bucket\n",
    "    for j in range(1,6):\n",
    "        num_words = len(pos_tab5.ix[(i,j)]['weight'])\n",
    "        perc = (num_words / (1.0 * tot_in_quantile)) * 100\n",
    "        print(\"Term Freq Quintile \" + str(j) + \": \" + str(perc))\n",
    "        \n",
    "print(\"\\nNegative Weight Quintiles by Term Frequency Quintiles\")\n",
    "for i in range(1,6):\n",
    "    tot_in_quantile = len(neg_tab5.ix[i]['weight'])\n",
    "    \n",
    "    print(\"\\nWeight Quintile \" + str(i) + \": Total words = \" + str(tot_in_quantile))\n",
    "    \n",
    "    for j in range(1,6):\n",
    "        num_words = len(neg_tab5.ix[(i,j)]['weight'])\n",
    "        perc = (num_words / (1.0 * tot_in_quantile)) * 100\n",
    "        print(\"Term Freq Quintile \" + str(j) + \": \" + str(perc))\n",
    "\n",
    "\n",
    "# Now, sort by the frequency buckets (ascending) and weights (descending)\n",
    "pos_coeffs.sort_values(['bucket_freq','weight'], ascending=[True,False], inplace=True)\n",
    "neg_coeffs.sort_values(['bucket_freq','weight'], ascending=[True,False], inplace=True)\n",
    "\n",
    "# Get the top 5 words in each quintile\n",
    "pos_top5 = pos_coeffs.reset_index(level=['word']).set_index(['bucket_freq'])\n",
    "pos_top5_list = {}\n",
    "neg_top5 = neg_coeffs.reset_index(level=['word']).set_index(['bucket_freq'])\n",
    "neg_top5_list = {}\n",
    "for i in range(1, 6):\n",
    "    pos_words = []\n",
    "    pos_bucket_words = pos_top5.ix[i]['word']\n",
    "    neg_words = []\n",
    "    neg_bucket_words = neg_top5.ix[i]['word']\n",
    "    \n",
    "    count = 0\n",
    "    for word in pos_bucket_words:\n",
    "        if count >= 5:\n",
    "            break\n",
    "        pos_words.append(word)\n",
    "        count += 1\n",
    "    count = 0\n",
    "    for word in neg_bucket_words:\n",
    "        if count >= 5:\n",
    "            break\n",
    "        neg_words.append(word)\n",
    "        count += 1\n",
    "    \n",
    "    pos_top5_list[i] = pos_words\n",
    "    neg_top5_list[i] = neg_words\n",
    "\n",
    "# Print out most frequent positive words\n",
    "header = \"Top Five Most Positive and Negative Words within Frequency Quintiles\"\n",
    "print(header)\n",
    "print(\"=\"*len(header))\n",
    "\n",
    "subheader = \"\\nTop 5 Most Positive Words\"\n",
    "print(subheader)\n",
    "print(\"=\"*len(subheader))\n",
    "\n",
    "for (bucket, words) in pos_top5_list.items():\n",
    "    print(\"\\nBucket \" + str(bucket))\n",
    "    for word in words:\n",
    "        print(word)\n",
    "\n",
    "# Print out most frequent negative words\n",
    "subheader = \"\\nTop 5 Most Negative Words\"\n",
    "print(subheader)\n",
    "print(\"=\"*len(subheader))\n",
    "\n",
    "for (bucket, words) in neg_top5_list.items():\n",
    "    print(\"\\nBucket \" + str(bucket))\n",
    "    for word in words:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Word Appearances\n",
      "                 apps                                              files\n",
      "able            33550  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
      "abundance         885  {2008:1060, 2006:512, 1996:977, 2000:71, 2000:...\n",
      "acclaimed         156  {2006:351, 2006:298, 2005:2315, 1997:1681, 200...\n",
      "accomplish      11726  {2001:3016, 2006:159, 1996:30, 1998:2882, 1999...\n",
      "achieve         34171  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
      "adequately      15393  {2000:2492, 1998:2882, 2001:1366, 2005:1378, 2...\n",
      "advancement     22713  {1996:30, 2006:2669, 2001:1366, 2005:1378, 200...\n",
      "advantage       29132  {1996:30, 2006:2669, 2001:1366, 2005:1378, 200...\n",
      "alliance        13832  {2001:3016, 2003:2411, 2006:159, 2005:1552, 20...\n",
      "assure          21675  {2003:2411, 2006:159, 2000:2492, 1998:2882, 20...\n",
      "attain          11135  {1998:2882, 2006:2669, 2005:1378, 2008:2218, 2...\n",
      "attractive      14756  {2005:1725, 2005:1552, 2006:2669, 2005:1378, 2...\n",
      "beautiful         247  {2005:1145, 2007:2562, 2005:1592, 1997:551, 19...\n",
      "beneficial      40015  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
      "bolstered         411  {2003:155, 2007:370, 2003:855, 2001:1676, 2004...\n",
      "boom              459  {2008:800, 2006:736, 2000:2941, 2000:3088, 199...\n",
      "boost            1088  {2007:2512, 2006:165, 2008:1060, 2006:2738, 20...\n",
      "breakthrough     1101  {2008:610, 2003:155, 2007:1345, 1999:2463, 200...\n",
      "brilliant          88  {2001:1759, 2000:2520, 2000:1903, 2001:1043, 2...\n",
      "charitable       3189  {2006:165, 2000:3426, 2007:2679, 2003:2330, 20...\n",
      "collaborate      8736  {2006:159, 1996:30, 2004:1600, 2005:1378, 2007...\n",
      "compliment       2180  {1998:3981, 2000:885, 2005:635, 2001:2797, 200...\n",
      "conclusive       5603  {2001:3016, 1996:30, 2005:1378, 2008:2218, 200...\n",
      "conducive         546  {2007:850, 2006:799, 2006:1756, 2001:2797, 200...\n",
      "confident        1111  {2006:799, 2003:2117, 1998:2882, 2006:2139, 20...\n",
      "constructive     1454  {2003:2411, 2007:2809, 2006:2579, 1998:2286, 2...\n",
      "courteous         369  {1997:3426, 2005:635, 2007:188, 2007:437, 1997...\n",
      "creative         5276  {2003:2411, 1998:3694, 1996:30, 2002:745, 2006...\n",
      "delight           166  {1999:1768, 1999:2862, 1998:407, 2002:2264, 20...\n",
      "dependability    1973  {2000:2492, 2001:1366, 2007:1847, 1997:1681, 2...\n",
      "...               ...                                                ...\n",
      "profitability   30852  {2006:2669, 2001:1366, 2005:1378, 2007:460, 19...\n",
      "progress        20912  {2006:159, 2005:1725, 2000:2492, 1998:2882, 20...\n",
      "prospered         203  {2007:1593, 2007:1050, 2001:473, 2002:804, 199...\n",
      "rebound          1029  {2004:2862, 1998:690, 2007:1684, 2003:1976, 20...\n",
      "receptive         662  {2000:3182, 2004:1402, 2000:579, 2006:512, 200...\n",
      "regain           1144  {1998:2882, 1999:242, 2001:3036, 2002:2413, 20...\n",
      "resolve          9099  {2006:159, 2001:2771, 2005:1378, 2008:2218, 19...\n",
      "revolutionize     333  {2007:2562, 2000:2853, 2001:1570, 1999:3144, 1...\n",
      "reward           6251  {2006:159, 2000:2492, 2005:1552, 2006:2669, 20...\n",
      "satisfactorily  32115  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
      "smooth           2477  {1998:2882, 2001:1689, 2005:1378, 2003:2624, 1...\n",
      "solves           1465  {2005:977, 1998:3981, 2001:2797, 2003:578, 200...\n",
      "spectacular       119  {1999:536, 2005:2485, 1999:1041, 2000:2795, 20...\n",
      "stability       16820  {2001:198, 2000:3604, 2004:1600, 2006:2669, 20...\n",
      "strength        23987  {1996:30, 2001:1366, 2005:1378, 2007:460, 1996...\n",
      "strong          26110  {2006:2669, 2001:1366, 2007:460, 1996:1353, 19...\n",
      "succeed         35744  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
      "superior        18562  {2003:2411, 2006:159, 2005:1725, 2001:198, 199...\n",
      "surpass          1080  {1996:1305, 2003:1112, 2006:512, 2008:610, 199...\n",
      "transparency     1156  {2004:2416, 1999:2087, 2006:468, 1998:3981, 20...\n",
      "tremendous        860  {2000:3604, 2002:2257, 1999:122, 2007:370, 199...\n",
      "unmatched         419  {2004:2416, 2006:165, 2002:2460, 2000:1327, 20...\n",
      "unparalleled      351  {2005:819, 2002:1274, 2002:2413, 1998:3328, 20...\n",
      "unsurpassed       124  {2005:104, 1998:2851, 2008:122, 2002:1453, 200...\n",
      "upturn            691  {2005:977, 2000:247, 2005:1725, 2007:370, 2006...\n",
      "valuable         8810  {2003:2411, 2001:1366, 2007:460, 2008:2218, 20...\n",
      "versatile        2351  {2002:745, 1997:1681, 1997:3367, 2003:578, 200...\n",
      "vibrant           276  {2005:2826, 2001:2394, 2005:2485, 2007:2471, 2...\n",
      "winning          4190  {2006:165, 1997:799, 1998:562, 2001:1689, 2003...\n",
      "worthy            378  {2008:1433, 2006:495, 2003:2533, 2001:607, 199...\n",
      "\n",
      "[123 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(40946,\n",
       "                  apps                                              files\n",
       " able            33550  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " abundance         885  {2008:1060, 2006:512, 1996:977, 2000:71, 2000:...\n",
       " acclaimed         156  {2006:351, 2006:298, 2005:2315, 1997:1681, 200...\n",
       " accomplish      11726  {2001:3016, 2006:159, 1996:30, 1998:2882, 1999...\n",
       " achieve         34171  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " adequately      15393  {2000:2492, 1998:2882, 2001:1366, 2005:1378, 2...\n",
       " advancement     22713  {1996:30, 2006:2669, 2001:1366, 2005:1378, 200...\n",
       " advantage       29132  {1996:30, 2006:2669, 2001:1366, 2005:1378, 200...\n",
       " alliance        13832  {2001:3016, 2003:2411, 2006:159, 2005:1552, 20...\n",
       " assure          21675  {2003:2411, 2006:159, 2000:2492, 1998:2882, 20...\n",
       " attain          11135  {1998:2882, 2006:2669, 2005:1378, 2008:2218, 2...\n",
       " attractive      14756  {2005:1725, 2005:1552, 2006:2669, 2005:1378, 2...\n",
       " beautiful         247  {2005:1145, 2007:2562, 2005:1592, 1997:551, 19...\n",
       " beneficial      40015  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " bolstered         411  {2003:155, 2007:370, 2003:855, 2001:1676, 2004...\n",
       " boom              459  {2008:800, 2006:736, 2000:2941, 2000:3088, 199...\n",
       " boost            1088  {2007:2512, 2006:165, 2008:1060, 2006:2738, 20...\n",
       " breakthrough     1101  {2008:610, 2003:155, 2007:1345, 1999:2463, 200...\n",
       " brilliant          88  {2001:1759, 2000:2520, 2000:1903, 2001:1043, 2...\n",
       " charitable       3189  {2006:165, 2000:3426, 2007:2679, 2003:2330, 20...\n",
       " collaborate      8736  {2006:159, 1996:30, 2004:1600, 2005:1378, 2007...\n",
       " compliment       2180  {1998:3981, 2000:885, 2005:635, 2001:2797, 200...\n",
       " conclusive       5603  {2001:3016, 1996:30, 2005:1378, 2008:2218, 200...\n",
       " conducive         546  {2007:850, 2006:799, 2006:1756, 2001:2797, 200...\n",
       " confident        1111  {2006:799, 2003:2117, 1998:2882, 2006:2139, 20...\n",
       " constructive     1454  {2003:2411, 2007:2809, 2006:2579, 1998:2286, 2...\n",
       " courteous         369  {1997:3426, 2005:635, 2007:188, 2007:437, 1997...\n",
       " creative         5276  {2003:2411, 1998:3694, 1996:30, 2002:745, 2006...\n",
       " delight           166  {1999:1768, 1999:2862, 1998:407, 2002:2264, 20...\n",
       " dependability    1973  {2000:2492, 2001:1366, 2007:1847, 1997:1681, 2...\n",
       " ...               ...                                                ...\n",
       " profitability   30852  {2006:2669, 2001:1366, 2005:1378, 2007:460, 19...\n",
       " progress        20912  {2006:159, 2005:1725, 2000:2492, 1998:2882, 20...\n",
       " prospered         203  {2007:1593, 2007:1050, 2001:473, 2002:804, 199...\n",
       " rebound          1029  {2004:2862, 1998:690, 2007:1684, 2003:1976, 20...\n",
       " receptive         662  {2000:3182, 2004:1402, 2000:579, 2006:512, 200...\n",
       " regain           1144  {1998:2882, 1999:242, 2001:3036, 2002:2413, 20...\n",
       " resolve          9099  {2006:159, 2001:2771, 2005:1378, 2008:2218, 19...\n",
       " revolutionize     333  {2007:2562, 2000:2853, 2001:1570, 1999:3144, 1...\n",
       " reward           6251  {2006:159, 2000:2492, 2005:1552, 2006:2669, 20...\n",
       " satisfactorily  32115  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " smooth           2477  {1998:2882, 2001:1689, 2005:1378, 2003:2624, 1...\n",
       " solves           1465  {2005:977, 1998:3981, 2001:2797, 2003:578, 200...\n",
       " spectacular       119  {1999:536, 2005:2485, 1999:1041, 2000:2795, 20...\n",
       " stability       16820  {2001:198, 2000:3604, 2004:1600, 2006:2669, 20...\n",
       " strength        23987  {1996:30, 2001:1366, 2005:1378, 2007:460, 1996...\n",
       " strong          26110  {2006:2669, 2001:1366, 2007:460, 1996:1353, 19...\n",
       " succeed         35744  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " superior        18562  {2003:2411, 2006:159, 2005:1725, 2001:198, 199...\n",
       " surpass          1080  {1996:1305, 2003:1112, 2006:512, 2008:610, 199...\n",
       " transparency     1156  {2004:2416, 1999:2087, 2006:468, 1998:3981, 20...\n",
       " tremendous        860  {2000:3604, 2002:2257, 1999:122, 2007:370, 199...\n",
       " unmatched         419  {2004:2416, 2006:165, 2002:2460, 2000:1327, 20...\n",
       " unparalleled      351  {2005:819, 2002:1274, 2002:2413, 1998:3328, 20...\n",
       " unsurpassed       124  {2005:104, 1998:2851, 2008:122, 2002:1453, 200...\n",
       " upturn            691  {2005:977, 2000:247, 2005:1725, 2007:370, 2006...\n",
       " valuable         8810  {2003:2411, 2001:1366, 2007:460, 2008:2218, 20...\n",
       " versatile        2351  {2002:745, 1997:1681, 1997:3367, 2003:578, 200...\n",
       " vibrant           276  {2005:2826, 2001:2394, 2005:2485, 2007:2471, 2...\n",
       " winning          4190  {2006:165, 1997:799, 1998:562, 2001:1689, 2003...\n",
       " worthy            378  {2008:1433, 2006:495, 2003:2533, 2001:607, 199...\n",
       " \n",
       " [123 rows x 2 columns],\n",
       "                   apps                                              files\n",
       " abandons          8053  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " abdications         33  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " aberrations        286  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " abetting           310  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " abnormally        5090  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " abolishing         290  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " abrogations        108  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " abruptness         615  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " absences         11886  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " absenteeism         99  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " abusiveness       3777  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " accidentally      7327  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " accusing           719  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " acquiescing         87  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " acquitting          41  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " adulterations      511  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " adversary          502  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " adversity        38266  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " aftermaths         507  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " against              0  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " aggravations       221  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " alerting          1914  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " alienations        204  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " alleging         22552  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " annoys              71  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " annuls             119  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " anomaly            657  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " anticompetitive      0  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " antitrust         3980  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " arguments         2664  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " ...                ...                                                ...\n",
       " unwarranted        238  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " unwelcome           17  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " unwillingness     3511  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " upset              120  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " urgent             645  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " usurious           196  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " usurps              38  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " vandalism          567  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " verdicts          1936  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " vetoed              69  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " victims            360  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " violators        19808  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " violently          890  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " vitiation           11  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " voiding            450  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " volatility       32557  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " vulnerably        8640  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " warns             4142  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " wasting           2482  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " weakens           6332  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " weaknesses       15952  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " willfully         1079  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " worrying            66  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " worsens           1875  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " worst            17014  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " worthless          449  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " writedowns           0  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " writeoffs            0  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " wrongdoings       1519  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " wrongly           3606  {2003:2411, 1996:30, 2006:2669, 2001:1366, 200...\n",
       " \n",
       " [718 rows x 2 columns])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def get_appearances():\n",
    "    try: yearly_data\n",
    "    except NameError: yearly_data = pickle.loads(zlib.decompress(rds.get(\"yearly-data\")))\n",
    "\n",
    "    pos_appearances_dict = defaultdict(int)\n",
    "    neg_appearances_dict = defaultdict(int)\n",
    "    pos_app_set = {}\n",
    "    neg_app_set = {}    \n",
    "\n",
    "    total = 0\n",
    "    for year in yearly_data:\n",
    "        total += len(yearly_data[year])\n",
    "\n",
    "        i = 0\n",
    "        for report in yearly_data[year]:\n",
    "            # Keep track if a word appears at least once and increment a counter if so\n",
    "            pos_occurs = report['pos_occurs']\n",
    "            neg_occurs = report['neg_occurs']\n",
    "\n",
    "            for (word, freq) in pos_occurs.items():\n",
    "                try: word_set = pos_app_set[word]\n",
    "                except: word_set = set()\n",
    "                    \n",
    "                if freq > 0:\n",
    "                    pos_appearances_dict[word] += 1\n",
    "                    word_set.add(str(year) + \":\" + str(i))\n",
    "                    pos_app_set[word] = word_set\n",
    "                else:\n",
    "                    pos_appearances_dict[word] += 0\n",
    "                    pos_app_set[word] = word_set\n",
    "                    \n",
    "            for (word, freq) in neg_occurs.items():\n",
    "                if freq > 0:\n",
    "                    neg_appearances_dict[word] += 1\n",
    "                    word_set.add(str(year) + \":\" + str(i))\n",
    "                    neg_app_set[word] = word_set\n",
    "                else:\n",
    "                    neg_appearances_dict[word] += 0\n",
    "                    neg_app_set[word] = word_set\n",
    "            i += 1\n",
    "\n",
    "    pos_appearances = pd.DataFrame({'apps': pos_appearances_dict, 'files': pos_app_set})\n",
    "    neg_appearances = pd.DataFrame({'apps': neg_appearances_dict, 'files': neg_app_set})\n",
    "    return (total, pos_appearances, neg_appearances)\n",
    "\n",
    "#get_appearances()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'total' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-7cf384c76fe8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_appearances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_appearances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'total' is not defined"
     ]
    }
   ],
   "source": [
    "del total, pos_appearances, neg_appearances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
