{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cython extension is already loaded. To reload it, use:\n",
      "  %reload_ext cython\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame, read_sas, read_csv\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from SECEdgar.crawler import SecCrawler\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "from datetime import date, timedelta\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import os\n",
    "import re\n",
    "import lxml\n",
    "import redis\n",
    "import string\n",
    "import pickle\n",
    "import math\n",
    "import zlib\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "try: stopwords.words('english')\n",
    "except LookupError: nltk.download('stopwords')\n",
    "    \n",
    "import statsmodels.api as sm\n",
    "    \n",
    "rds = redis.Redis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in SAS data set - takes a while so try to use redis...\n",
    "try: data\n",
    "except:\n",
    "    if rds.exists('data:word-power'):\n",
    "        %time data = pickle.loads(zlib.decompress(rds.get('data:word-power')))\n",
    "    else:\n",
    "        %time data = read_sas(\"data/crsp_comp.sas7bdat\")\n",
    "\n",
    "        # Trim the SAS data set\n",
    "        data = data[['CUSIP','PERMNO','cik','date','PRC','RET','vwretd']]\n",
    "\n",
    "        # Sort the set by cusip, permno, cik, and then year (descending)\n",
    "        data.sort_values(['CUSIP', 'PERMNO', 'cik', 'date'], ascending=[True, True, True, False], inplace=True)\n",
    "\n",
    "        # Re-index the dataframe for better access\n",
    "        data.reset_index(inplace=True)\n",
    "\n",
    "        rds.set('data:word-power', zlib.compress(pickle.dumps(data)))\n",
    "\n",
    "# We only need certain columns from the data set and we must set the right index for performance\n",
    "try: df\n",
    "except:\n",
    "    df = data[[\"cik\", \"date\", \"PRC\", \"RET\", \"vwretd\"]]\n",
    "    df.set_index(keys=['cik','date'], inplace=True)\n",
    "        \n",
    "# Positive words\n",
    "try: pos_dict, pos_roots, pos_roots_map\n",
    "except:\n",
    "    if rds.exists('data:pos-dict') and rds.exists('data:pos-roots') and rds.exists('data:pos-roots-map'):\n",
    "        pos_dict = pickle.loads(rds.get('data:pos-dict'))\n",
    "        pos_roots = pickle.loads(rds.get('data:pos-roots'))\n",
    "        pos_roots_map = pickle.loads(rds.get('data:pos-roots-map'))\n",
    "    else:\n",
    "        # Read in the positive word list(s)\n",
    "        pos_dict = read_csv(\"data/pos_list.csv\", header=None, names=['word'])\n",
    "        pos_dict = set(pos_dict['word'])\n",
    "        pos_roots = read_csv(\"data/pos_roots.csv\")\n",
    "        pos_roots_map = dict(zip(list(pos_roots.word), list(pos_roots.group)))\n",
    "        pos_roots = set(pos_roots['group'].drop_duplicates())\n",
    "\n",
    "        # Save this data to redis for later\n",
    "        rds.set('data:pos-dict', pickle.dumps(pos_dict))\n",
    "        rds.set('data:pos-roots', pickle.dumps(pos_roots))\n",
    "        rds.set('data:pos-roots-map', pickle.dumps(pos_roots_map))\n",
    "\n",
    "# Negative words\n",
    "try: neg_dict, neg_roots, neg_roots_map\n",
    "except:\n",
    "    if rds.exists('data:neg-dict') and rds.exists('data:neg-roots') and rds.exists('data:neg-roots-map'):\n",
    "        neg_dict = pickle.loads(rds.get('data:neg-dict'))\n",
    "        neg_roots = pickle.loads(rds.get('data:neg-roots'))\n",
    "        neg_roots_map = pickle.loads(rds.get('data:neg-roots-map'))\n",
    "    else:\n",
    "        # Read in the negative word list(s)\n",
    "        neg_dict = read_csv(\"data/neg_list.csv\", header=None, names=['word'])\n",
    "        neg_dict = set(neg_dict['word'])\n",
    "        neg_roots = read_csv(\"data/neg_roots.csv\")\n",
    "        neg_roots_map = dict(zip(list(neg_roots.word), list(neg_roots.group)))\n",
    "        neg_roots = set(neg_roots['group'].drop_duplicates())\n",
    "\n",
    "        # Save this data to redis for later\n",
    "        rds.set('data:neg-dict', pickle.dumps(neg_dict))\n",
    "        rds.set('data:neg-roots', pickle.dumps(neg_roots))\n",
    "        rds.set('data:neg-roots-map', pickle.dumps(neg_roots_map))\n",
    "\n",
    "# 2of12inf dictionary\n",
    "try: dict_2of12inf\n",
    "except:\n",
    "    if rds.exists('data:2of12inf'):\n",
    "        dict_2of12inf = pickle.loads(rds.get('data:2of12inf'))\n",
    "    else:\n",
    "        # Read in the 2of12inf\n",
    "        dict_2of12inf = read_csv(\"data/2of12inf.txt\", header=None, names=['word'])\n",
    "\n",
    "        # Iterate through and remove the percent signs\n",
    "        regex = re.compile(r'%$')\n",
    "        dict_2of12inf.apply(lambda x: re.sub(regex, r'', x['word']), axis=1)\n",
    "        dict_2of12inf = set(dict_2of12inf['word'])\n",
    "\n",
    "        # Save this to redis for later\n",
    "        rds.set('data:2of12inf', pickle.dumps(dict_2of12inf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't find stock data\n"
     ]
    }
   ],
   "source": [
    "cik = \"0000884219\"\n",
    "filing_dt = dt(1999, 3, 24)\n",
    "cik_df = df.ix[bytes(cik, 'utf-8')]\n",
    "try: \n",
    "    index = df.index.get_loc((bytes(cik, 'utf-8'), filing_dt))\n",
    "except (IndexError, KeyError):\n",
    "    print(\"Can't find stock data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "index = b'1110776400.0'\n",
    "index = int(float(index.decode('utf-8')))\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Remove any duplicates where CUSIP, PERMNO, and CIK match\n",
    "ciks = data.drop_duplicates(subset=['CUSIP', 'PERMNO', 'cik'])\n",
    "\n",
    "# Only keep the cik and ticker column\n",
    "ciks = ciks[['cik', 'tic']]\n",
    "\n",
    "# Iterate over each CIK and pull the relevant 10k filings\n",
    "crawler = SecCrawler()\n",
    "end_date = '20081231'\n",
    "count = '20'\n",
    "\n",
    "for index, row in ciks.iterrows():\n",
    "    cik = row.iloc[0]\n",
    "    tic = row.iloc[1]\n",
    "    crawler.filing_10K(tic, cik, end_date, count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "from cpython cimport bool\n",
    "\n",
    "from __main__ import rds\n",
    "\n",
    "def check_redis(str cleaned_key, str processed_key, str report_key):\n",
    "    cdef bool processed = False\n",
    "    cdef bool cleaned = False\n",
    "    cdef str mtime\n",
    "    \n",
    "    if not rds.exists(cleaned_key):\n",
    "        if not rds.exists(processed_key):\n",
    "            # Temporary check to see if this file has been processed fully\n",
    "            if rds.exists(report_key):\n",
    "                mtime = rds.hget(report_key, 'mtime')\n",
    "\n",
    "                if not rds.hexists(report_key, 'company_data'):\n",
    "                    # Hasn't been cleaned with the new algorithm, so keep booleans False\n",
    "                    pass\n",
    "                elif rds.hexists(report_key, 'hist_ret'):\n",
    "                    processed = True\n",
    "                    cleaned = True\n",
    "\n",
    "                    # Save to proper place in redis\n",
    "                    rds.set(cleaned_key, mtime)\n",
    "                    rds.set(processed_key, mtime)\n",
    "                else:\n",
    "                    cleaned = True\n",
    "\n",
    "                    # Save to proper place in redis\n",
    "                    rds.set(cleaned_key, mtime)\n",
    "        else:\n",
    "            processed = True\n",
    "    else:\n",
    "        # Check to see if this has really been cleaned (company_data exists)\n",
    "        if rds.hexists(report_key, 'company_data'):\n",
    "            cleaned = True\n",
    "            if rds.exists(processed_key):\n",
    "                processed = True\n",
    "        \n",
    "    return (cleaned, processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "import os\n",
    "\n",
    "def move_file(fh, str fn, str folder, str tic, str cik, str filename, str message):\n",
    "    # Generate the new name of the file\n",
    "    cdef str s = os.sep\n",
    "    cdef str new_name = 'data' + s + folder + s + tic + '-' + cik + '-' + filename\n",
    "\n",
    "    # Close the file so that we can move it\n",
    "    fh.close()\n",
    "    os.rename(fn, new_name)\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "import pickle\n",
    "from datetime import datetime as dt\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "from __main__ import move_file, df, stopwords, rds, dict_2of12inf\n",
    "\n",
    "from cpython cimport bool\n",
    "\n",
    "# This function handles the cleaning of the 10-K\n",
    "def clean(str fn):\n",
    "    cdef bool error = False\n",
    "    \n",
    "    cdef str s = os.sep\n",
    "    cdef str tic = fn.split(s)[1]\n",
    "    cdef str cik = fn.split(s)[2]\n",
    "    cdef str filename = fn.split(s)[4]\n",
    "    cdef str report_key = \"report:\" + cik + \":\" + fn\n",
    "    cdef str cleaned_key = \"cleaned:\" + cik + \":\" + fn\n",
    "\n",
    "    # Open the file, get all of the content, and then pull it into a parser\n",
    "    fh = open(fn, 'r')\n",
    "    cdef unicode contents = fh.read()\n",
    "\n",
    "    # Clean up some of the text to fix malformed HTML before parsing it\n",
    "    cdef list malformed_tags = ['ACCEPTANCE-DATETIME', 'TYPE', 'SEQUENCE', 'FILENAME', 'DESCRIPTION']\n",
    "    cdef str tag\n",
    "    for tag in malformed_tags:\n",
    "        # Do a regex that replaces all of these malformed tags in the document\n",
    "        regex = re.compile(r\"(\\n<%s>[^<]*?)\\n\" % re.escape(tag), re.I)\n",
    "        contents = regex.sub(r\"\\1</%s>\\n\" % tag, contents)\n",
    "\n",
    "    # Pull the 10-k into the parser\n",
    "    document = bs(contents, 'lxml')\n",
    "\n",
    "    # The document can either have a root node of sec-document or ims-document\n",
    "    if document.find('sec-document') is not None:\n",
    "        root = document.find('sec-document')\n",
    "    elif document.find('ims-document') is not None: \n",
    "        root = document.find('ims-document')\n",
    "    elif document.find('document') is not None:\n",
    "        root = document.find('document')\n",
    "    elif document.find('error') is not None:\n",
    "        root = None\n",
    "    else:\n",
    "        root = None\n",
    "        \n",
    "    if root is None:\n",
    "        # Root node error \n",
    "        move_file(fh, fn, \"_error\", tic, cik, filename, \"No root or erroneous root node - moved file\")\n",
    "        error = True\n",
    "    if error: return error\n",
    "        \n",
    "\n",
    "    # Check if this is an amended 10-K and throw it out if so\n",
    "    type_text = root.find('type')\n",
    "    if type_text is None:\n",
    "        move_file(fh, fn, \"_error\", tic, cik, filename, \"Error finding type - moved file\")\n",
    "        error = True\n",
    "    if error: return error\n",
    "\n",
    "    elif type_text.text == '10-K/A':\n",
    "        move_file(fh, fn, \"_amended\", tic, cik, filename, \"Amended 10-K - moved file\")\n",
    "        error = True\n",
    "    if error: return error\n",
    "\n",
    "    # Get the 'acceptance-datetime' metadata element\n",
    "    acc_dt = root.find('acceptance-datetime')\n",
    "    if acc_dt is None:\n",
    "        header_text = None\n",
    "        # If we didn't find an <acceptance-datetime /> element, find the date elsewhere\n",
    "        if root.find('sec-header') is not None:\n",
    "            header_text = root.find('sec-header').text\n",
    "        elif root.find('ims-header') is not None:\n",
    "            header_text = root.find('ims-header').text\n",
    "\n",
    "        if header_text:\n",
    "            regex = re.compile(r\".*\\nFILED AS OF DATE:\\s+?([\\d]+?)\\n.*\", re.S)\n",
    "            filing_dt_text = re.sub(regex, r\"\\1\", header_text)\n",
    "        else:\n",
    "            move_file(fh, fn, \"_error\", tic, cik, filename, \"Bad filing date - moved file\")\n",
    "            error = True\n",
    "        if error: return error\n",
    "    else:\n",
    "        # Get the filing date\n",
    "        filing_dt_text = acc_dt.text.split('\\n', 1)[0][:8]\n",
    "\n",
    "    filing_dt = dt.strptime(filing_dt_text, '%Y%m%d')\n",
    "    filing_ts = time.mktime(filing_dt.timetuple())\n",
    "    begin_dt = dt(1995, 1, 1)\n",
    "\n",
    "    # If the filing date is not within our date range, then move it\n",
    "    if begin_dt > filing_dt:\n",
    "        move_file(fh, fn, \"_outofrange\", tic, cik, filename, \"Out of date range - moved file.\")\n",
    "        error = True\n",
    "    if error: return error\n",
    "\n",
    "    # See if we can find stock info for this company on the filing date of the 10-K\n",
    "    cdef int index = 0\n",
    "    cik_df = None\n",
    "    try:\n",
    "        index = df.index.get_loc((bytes(cik, 'utf-8'), filing_dt))\n",
    "        cik_df = df.ix[bytes(cik, 'utf-8')]\n",
    "        price = cik_df.ix[filing_dt, 'PRC']\n",
    "        # Now, check if the price of the stock is less than $3.00\n",
    "        if price < 3.0:\n",
    "            move_file(fh, fn, \"_nostockdata\", tic, cik, filename, \"Price less than $3.00 - moved file.\")\n",
    "            error = True\n",
    "    except (IndexError, KeyError):\n",
    "        # We couldn't find the cik or date for this 10-k\n",
    "        move_file(fh, fn, \"_nostockdata\", tic, cik, filename, \"No stock data found - moved file.\")\n",
    "        error = True\n",
    "    if error: return error\n",
    "    \n",
    "    # Remove the exhibits\n",
    "    [ex.extract() for ex in root.findAll('document')[1:]]\n",
    "\n",
    "    # Grab the report\n",
    "    cdef str report = root.find('text').get_text()\n",
    "\n",
    "    # We will tokenize the text and iterate through each word\n",
    "    cdef list tokens = report.split()\n",
    "    cdef list keep_tokens = []\n",
    "    cdef set stopwords_set = set(stopwords.words('english'))\n",
    "    punc_table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    \n",
    "    # Filter out words\n",
    "    cdef str word\n",
    "    for word in tokens:\n",
    "        # Quick check to make sure we should keep filtering the word\n",
    "        if len(word) != 1:\n",
    "            # Strip punctuation from the word first and make it lowercase\n",
    "            word = word.translate(punc_table).lower()\n",
    "\n",
    "            # Add the word to the keep pile if it is not a stopword and if it is in 2of12inf dictionary\n",
    "            if word not in stopwords_set and word in dict_2of12inf:\n",
    "                keep_tokens.append(word)\n",
    "            \n",
    "    tokens = keep_tokens\n",
    "    report = \" \".join(tokens)\n",
    "    cdef int total_words = len(tokens)\n",
    "\n",
    "    # Gather info for report to save into redis\n",
    "    report_hash = {\n",
    "        'cik': cik,\n",
    "        'tic': tic,\n",
    "        'path': fn,\n",
    "        'file_name': filename,\n",
    "        'filing_date': filing_ts,\n",
    "        'year': filing_dt.year,\n",
    "        'report': report,\n",
    "        'total_words': total_words,\n",
    "        'company_data': pickle.dumps(cik_df),\n",
    "        'index': index,\n",
    "        'mtime': time.time()\n",
    "    }\n",
    "\n",
    "    # Close the file handle\n",
    "    fh.close()\n",
    "    \n",
    "    # Save the stuff to redis\n",
    "    print(\"Saving to redis: \" + report_key)\n",
    "    rds.hmset(report_key, report_hash)\n",
    "    rds.set(cleaned_key, time.time())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "import os\n",
    "import pickle\n",
    "import math\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from __main__ import df, rds, pos_dict, pos_roots, pos_roots_map, neg_dict, neg_roots, neg_roots_map\n",
    "\n",
    "from cpython cimport bool\n",
    "\n",
    "def process(str fn):\n",
    "    \n",
    "    cdef str s = os.sep\n",
    "    cdef str tic = fn.split(s)[1]\n",
    "    cdef str cik = fn.split(s)[2]\n",
    "    cdef str report_key = \"report:\" + cik + \":\" + fn\n",
    "    cdef str processed_key = \"processed:\"+ cik + \":\" + fn\n",
    "    \n",
    "    # Get the report out of redis\n",
    "    #print(\"Found in redis: \" + report_key)\n",
    "    cdef str report = str(rds.hget(report_key, 'report'))\n",
    "    filing_dt = dt.fromtimestamp(int(float(rds.hget(report_key, 'filing_date').decode('utf-8'))))\n",
    "    cik_df = pickle.loads(rds.hget(report_key, 'company_data'))\n",
    "    cdef int index = int(rds.hget(report_key, 'index'))\n",
    "    cdef dict report_hash = {}\n",
    "\n",
    "    # Now that everything is cleaned up, we can run the word processing algorithm\n",
    "    pos_occurs = defaultdict(int)\n",
    "    neg_occurs = defaultdict(int)\n",
    "    negators = pd.Series(['not', 'no', 'never'])\n",
    "\n",
    "    # We will tokenize the text and iterate through each word\n",
    "    tokens = pd.Series(report.split())\n",
    "\n",
    "    # Now, process the text\n",
    "    cdef int i\n",
    "    cdef str token, root, word\n",
    "    cdef bool negated\n",
    "    for i, token in tokens.iteritems():\n",
    "        if token in pos_dict:\n",
    "            # Check to see if there is a negator\n",
    "            negated = False\n",
    "            for word in tokens.iloc[(i - 3):(i + 3)]:\n",
    "                if word in negators.values:\n",
    "                    #print(\"Found a negator: \" + word + \" - \" + token)\n",
    "                    negated = True\n",
    "            if not negated:\n",
    "                root = pos_roots_map[token]\n",
    "                pos_occurs[root] += 1\n",
    "        elif token in neg_dict:\n",
    "            # Check to see if there is a negator\n",
    "            negated = False\n",
    "            for word in tokens.iloc[(i - 3):(i + 3)]:\n",
    "                if word in negators.values:\n",
    "                    #print(\"Found a negator: \" + word + \" - \" + token)\n",
    "                    negated = True\n",
    "            if not negated:\n",
    "                root = neg_roots_map[token]\n",
    "                neg_occurs[root] += 1\n",
    "\n",
    "    # For the roots we didn't find, set frequency to zero\n",
    "    for root in pos_roots:\n",
    "        if root not in pos_occurs:\n",
    "            pos_occurs[root] = 0\n",
    "    for root in neg_roots:\n",
    "        if root not in neg_occurs:\n",
    "            neg_occurs[root] = 0\n",
    "            \n",
    "    # Use the index we found earlier to grab the historical info\n",
    "    hist_returns = cik_df.ix[(index + 1):, 'RET']\n",
    "    #hist_returns = cik_df.ix[cik_df.index > index]\n",
    "    #hist_returns = hist_returns[['RET']]\n",
    "\n",
    "    # Calculate the historical return before the filing date\n",
    "    cdef float hist_ret = 1.0\n",
    "    for col, series in hist_returns.iteritems():\n",
    "        if col == 'RET':\n",
    "            for r in series:\n",
    "                if not math.isnan(r):\n",
    "                    hist_ret *= (r + 1.0)\n",
    "    hist_ret = hist_ret - 1.0\n",
    "    #print(\"Historical return: \" + str(hist_ret))\n",
    "\n",
    "    # Use the index we found earlier to grab the four day window returns\n",
    "    returns = cik_df.ix[(index - 3):(index + 1), ['RET','vwretd']]\n",
    "    #returns = cik_df.ix[index:(index + 3)]\n",
    "    #returns = returns[['RET', 'vwretd']]\n",
    "\n",
    "    # Calculate the abnormal return: r_i = M{t=0, 3} (ret_i,j) - M{t=0,3} (ret_vwi,t)\n",
    "    cdef float ret = 1.0\n",
    "    cdef float ret_vwi = 1.0\n",
    "    for col, series in returns.iteritems():\n",
    "        if col == 'RET':\n",
    "            for r in series:\n",
    "                if not math.isnan(r):\n",
    "                    ret *= (r + 1.0)\n",
    "        elif col == 'vwretd':\n",
    "            for r in series:\n",
    "                if  not math.isnan(r):\n",
    "                    ret_vwi *= (r + 1.0)\n",
    "    cdef float ab_ret = ((ret - 1.0) - (ret_vwi - 1.0))\n",
    "    #print(\"Abnormal return: \" + str(ab_ret))\n",
    "\n",
    "    # Save results of text processing to key in redis\n",
    "    report_hash['pos_occurs'] = pickle.dumps(pos_occurs)\n",
    "    report_hash['neg_occurs'] = pickle.dumps(neg_occurs)\n",
    "    report_hash['hist_ret'] = hist_ret\n",
    "    report_hash['ab_ret'] = ab_ret\n",
    "    report_hash['mtime'] = time.time()\n",
    "\n",
    "    print(\"Saving to redis: \" + report_key)\n",
    "    rds.hmset(report_key, report_hash)\n",
    "    rds.set(processed_key, time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to redis: report:0000884219:SEC-Edgar-data/VVI/0000884219/10-K/0000950124-05-001493.txt\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable      Type    Data/Info\n",
      "-------------------------------\n",
      "F             int     0\n",
      "a             int     1801\n",
      "count         int     27646\n",
      "end           int     2008\n",
      "index         int     1110776400\n",
      "start         int     1997\n",
      "stop          int     100000\n",
      "t             int     1997\n",
      "total_words   int     11408\n",
      "year          int     1996\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) Cleaning SEC-Edgar-data/VVI/0000884219/10-K/0000950153-08-000420.txt\n",
      "Saving to redis: report:0000884219:SEC-Edgar-data/VVI/0000884219/10-K/0000950153-08-000420.txt\n",
      "(1) Processing SEC-Edgar-data/VVI/0000884219/10-K/0000950153-08-000420.txt\n",
      "test\n",
      "(2) Cleaning SEC-Edgar-data/VVI/0000884219/10-K/0000950153-06-000542.txt\n",
      "Saving to redis: report:0000884219:SEC-Edgar-data/VVI/0000884219/10-K/0000950153-06-000542.txt\n",
      "(2) Processing SEC-Edgar-data/VVI/0000884219/10-K/0000950153-06-000542.txt\n",
      "test\n",
      "(3) Cleaning SEC-Edgar-data/VVI/0000884219/10-K/0000884219-94-000033.txt\n",
      "Out of date range - moved file.\n",
      "(3) Cleaning SEC-Edgar-data/VVI/0000884219/10-K/0000950153-07-000453.txt\n",
      "Saving to redis: report:0000884219:SEC-Edgar-data/VVI/0000884219/10-K/0000950153-07-000453.txt\n",
      "(3) Processing SEC-Edgar-data/VVI/0000884219/10-K/0000950153-07-000453.txt\n",
      "test\n",
      "(4) Cleaning SEC-Edgar-data/VVI/0000884219/10-K/0000884219-98-000013.txt\n",
      "No stock data found - moved file.\n",
      "(4) Cleaning SEC-Edgar-data/VVI/0000884219/10-K/0000884219-95-000022.txt\n",
      "Bad filing date - moved file\n",
      "(4) Cleaning SEC-Edgar-data/VVI/0000884219/10-K/0000950153-03-001134.txt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-163-cc81d6c94e0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cython'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'import os\\n\\nfrom __main__ import check_redis, clean, process\\n\\nfrom cpython cimport bool\\n\\n# This is for testing\\ncdef int count = 1\\ncdef int stop = 100000\\ncdef bool skip_cleaned = True\\ncdef bool skip_processed = True\\ncdef bool process_file = True\\n\\ncdef dict report_hash\\ncdef str fn, s, tic, cik, cleaned_key, processed_key, report_key\\ncdef bool cleaned, processed, error\\n\\ncdef str dirpath\\ncdef list dirnames, filenames\\ncdef str folder = \"SEC-Edgar-data\"\\nfor (dirpath, dirnames, filenames) in os.walk(folder, topdown=False):\\n    for filename in filenames:                \\n        report_hash = {}\\n        fn = os.sep.join([dirpath, filename])\\n        \\n        if filename.endswith(\\'.txt\\'):# and filename == \"0000950116-97-000637.txt\":\\n            if count > stop:\\n                break\\n            \\n            s = os.sep\\n            tic = fn.split(s)[1]\\n            cik = fn.split(s)[2]\\n            \\n            # Check redis to see if we have processed or cleaned the report already\\n            cleaned_key = \"cleaned:\" + cik + \":\" + fn\\n            processed_key = \"processed:\" + cik + \":\" + fn\\n            report_key = \"report:\" + cik + \":\" + fn\\n            (cleaned, processed) = check_redis(cleaned_key, processed_key, report_key)\\n            \\n            # If the report has been cleaned or we don\\'t want to clean it anyway, skip this step\\n            error = False\\n            if not cleaned or not skip_cleaned:\\n                print(\"(\" + str(count) + \") Cleaning \" + fn)\\n                error = clean(fn)\\n                \\n                if not process and not error:\\n                    count += 1\\n                    continue\\n            if error: continue\\n            \\n            # After possibly cleaning, check if we should process the file\\n            if (not processed or not skip_processed) and process_file:\\n                print(\"(\" + str(count) + \") Processing \" + fn)\\n                process(fn)\\n                \\n                count += 1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2113\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-124>\u001b[0m in \u001b[0;36mcython\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/Cython/Build/IpythonMagic.py\u001b[0m in \u001b[0;36mcython\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_code_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_dynamic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_import_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/imp.py\u001b[0m in \u001b[0;36mload_dynamic\u001b[0;34m(name, path, file)\u001b[0m\n\u001b[1;32m    340\u001b[0m         spec = importlib.machinery.ModuleSpec(\n\u001b[1;32m    341\u001b[0m             name=name, loader=loader, origin=path)\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/importlib/_bootstrap.py\u001b[0m in \u001b[0;36mmodule_from_spec\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mcreate_module\u001b[0;34m(self, spec)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[0;32m_cython_magic_4d2f2557236cec10b306619ed7e53f81.pyx\u001b[0m in \u001b[0;36minit _cython_magic_4d2f2557236cec10b306619ed7e53f81 (/usr/home/andrew/.cache/ipython/cython/_cython_magic_4d2f2557236cec10b306619ed7e53f81.c:1968)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_cython_magic_ad730953ef7249b504a84515aedac3fe.pyx\u001b[0m in \u001b[0;36m_cython_magic_ad730953ef7249b504a84515aedac3fe.clean (/usr/home/andrew/.cache/ipython/cython/_cython_magic_ad730953ef7249b504a84515aedac3fe.c:1882)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mParserRejectedMarkup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m_feed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0;31m# Close out any unfinished strings and close all the open tags.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/bs4/builder/_lxml.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, markup)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mUnicodeDecodeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParserError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parser.pxi\u001b[0m in \u001b[0;36mlxml.etree._FeedParser.feed (src/lxml/lxml.etree.c:112202)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parser.pxi\u001b[0m in \u001b[0;36mlxml.etree._FeedParser.feed (src/lxml/lxml.etree.c:112077)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parsertarget.pxi\u001b[0m in \u001b[0;36mlxml.etree._TargetParserContext._handleParseResult (src/lxml/lxml.etree.c:128526)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parsertarget.pxi\u001b[0m in \u001b[0;36mlxml.etree._TargetParserContext._handleParseResult (src/lxml/lxml.etree.c:128396)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/lxml.etree.pyx\u001b[0m in \u001b[0;36mlxml.etree._ExceptionContext._raise_if_stored (src/lxml/lxml.etree.c:10741)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/saxparser.pxi\u001b[0m in \u001b[0;36mlxml.etree._handleSaxTargetStartNoNs (src/lxml/lxml.etree.c:121065)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/saxparser.pxi\u001b[0m in \u001b[0;36mlxml.etree._callTargetSaxStart (src/lxml/lxml.etree.c:121259)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parsertarget.pxi\u001b[0m in \u001b[0;36mlxml.etree._PythonSaxParserTarget._handleSaxStart (src/lxml/lxml.etree.c:127508)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/bs4/builder/_lxml.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, name, attrs, nsmap)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnsmaps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT_NSMAPS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnsmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0;31m# Make sure attrs is a mutable dict--lxml may send an immutable dictproxy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mattrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%cython\n",
    "import os\n",
    "\n",
    "from __main__ import check_redis, clean, process\n",
    "\n",
    "from cpython cimport bool\n",
    "\n",
    "# This is for testing\n",
    "cdef int count = 1\n",
    "cdef int stop = 100000\n",
    "cdef bool skip_cleaned = True\n",
    "cdef bool skip_processed = True\n",
    "cdef bool process_file = True\n",
    "\n",
    "cdef dict report_hash\n",
    "cdef str fn, s, tic, cik, cleaned_key, processed_key, report_key\n",
    "cdef bool cleaned, processed, error\n",
    "\n",
    "cdef str dirpath\n",
    "cdef list dirnames, filenames\n",
    "cdef str folder = \"SEC-Edgar-data\"\n",
    "for (dirpath, dirnames, filenames) in os.walk(folder, topdown=False):\n",
    "    for filename in filenames:                \n",
    "        report_hash = {}\n",
    "        fn = os.sep.join([dirpath, filename])\n",
    "        \n",
    "        if filename.endswith('.txt'):# and filename == \"0000950116-97-000637.txt\":\n",
    "            if count > stop:\n",
    "                break\n",
    "            \n",
    "            s = os.sep\n",
    "            tic = fn.split(s)[1]\n",
    "            cik = fn.split(s)[2]\n",
    "            \n",
    "            # Check redis to see if we have processed or cleaned the report already\n",
    "            cleaned_key = \"cleaned:\" + cik + \":\" + fn\n",
    "            processed_key = \"processed:\" + cik + \":\" + fn\n",
    "            report_key = \"report:\" + cik + \":\" + fn\n",
    "            (cleaned, processed) = check_redis(cleaned_key, processed_key, report_key)\n",
    "            \n",
    "            # If the report has been cleaned or we don't want to clean it anyway, skip this step\n",
    "            error = False\n",
    "            if not cleaned or not skip_cleaned:\n",
    "                print(\"(\" + str(count) + \") Cleaning \" + fn)\n",
    "                error = clean(fn)\n",
    "                \n",
    "                if not process and not error:\n",
    "                    count += 1\n",
    "                    continue\n",
    "            if error: continue\n",
    "            \n",
    "            # After possibly cleaning, check if we should process the file\n",
    "            if (not processed or not skip_processed) and process_file:\n",
    "                print(\"(\" + str(count) + \") Processing \" + fn)\n",
    "                process(fn)\n",
    "                \n",
    "                count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "stop = math.inf\n",
    "yearly_data = {}\n",
    "\n",
    "rds = redis.Redis()\n",
    "keys = rds.keys(\"report:*\")\n",
    "for key in keys:\n",
    "    \n",
    "    if count >= stop:\n",
    "        break\n",
    "        \n",
    "    report_hash = rds.hgetall(key)\n",
    "    try:\n",
    "        year = 1\n",
    "        pos_occurs = pickle.loads(report_hash[b'pos_occurs'])\n",
    "        neg_occurs = pickle.loads(report_hash[b'neg_occurs'])\n",
    "        year = int(report_hash[b'year'])\n",
    "        total_words = int(report_hash[b'total_words'])\n",
    "        hist_ret = float(report_hash[b'hist_ret'])\n",
    "        ab_ret = float(report_hash[b'ab_ret'])\n",
    "        \n",
    "        try: yearly_data[year]\n",
    "        except KeyError:\n",
    "            yearly_data[year] = []\n",
    "            \n",
    "        year_list = yearly_data[year]\n",
    "        year_list.append({\n",
    "            'pos_occurs': pos_occurs,\n",
    "            'neg_occurs': neg_occurs,\n",
    "            'total_words': total_words,\n",
    "            'hist_ret': hist_ret,\n",
    "            'ab_ret': ab_ret\n",
    "        })\n",
    "        yearly_data[year] = year_list\n",
    "        \n",
    "        count += 1\n",
    "    except KeyError:\n",
    "        continue\n",
    "    except e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for year in sorted(yearly_data.keys()):\n",
    "    print(year, len(yearly_data[year]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = 1997\n",
    "end = 2008\n",
    "# Generate a rolling training model using data up until year T-1\n",
    "for t in range(start, (end + 1)):\n",
    "    pos_word_weights = pd.DataFrame()\n",
    "    neg_word_weights = pd.DataFrame()\n",
    "    hist_returns = pd.DataFrame()\n",
    "    ab_returns = pd.DataFrame()\n",
    "    \n",
    "    # Iterate over each year before year T and build the training data set\n",
    "    for year in range((start - 1), t):\n",
    "        \n",
    "        try: yearly_data[year]\n",
    "        except KeyError:\n",
    "            print(\"Year \" + str(year) + \" not found.\")\n",
    "            continue\n",
    "        \n",
    "        # Iterate through each 10-K info for the year and generate the dataframe for the regression\n",
    "        for report in yearly_data[year]:\n",
    "            a = report['total_words']\n",
    "            hist_ret = report['hist_ret']\n",
    "            ab_ret = report['ab_ret']\n",
    "            \n",
    "            weights = {}\n",
    "            pos_occurs = report['pos_occurs']\n",
    "            for word in pos_occurs.keys():\n",
    "                F = pos_occurs[word]\n",
    "                weights[word] = F/(a * 1.0)\n",
    "            pos_word_weights = pos_word_weights.append(weights, ignore_index=True)\n",
    "            \n",
    "            weights = {}\n",
    "            neg_occurs = report['neg_occurs']\n",
    "            for word in neg_occurs.keys():\n",
    "                F = neg_occurs[word]\n",
    "                weights[word] = F/(a * 1.0)\n",
    "            neg_word_weights = neg_word_weights.append(weights, ignore_index=True)\n",
    "            \n",
    "            hist_returns = hist_returns.append({'hist_ret': hist_ret}, ignore_index=True)\n",
    "            ab_returns = ab_returns.append({'ab_ret': ab_ret}, ignore_index=True)\n",
    "        \n",
    "    # Run the regressions for this \n",
    "    if not hist_returns.empty and not pos_word_weights.empty and not neg_word_weights.empty:\n",
    "        hist_returns.reset_index()\n",
    "        hist_returns_series = pd.Series(hist_returns['hist_ret'])\n",
    "        ab_returns.reset_index()\n",
    "        ab_returns_series = pd.Series(ab_returns['ab_ret'])\n",
    "        pos_word_weights.reset_index()\n",
    "        neg_word_weights.reset_index()\n",
    "        \n",
    "        # Estimate the weights for the words using a regression\n",
    "        pos_reg = sm.OLS(hist_returns_series, pos_word_weights)\n",
    "        pos_model = pos_reg.fit()\n",
    "        neg_reg = sm.OLS(hist_returns_series, neg_word_weights)\n",
    "        neg_model = neg_reg.fit()\n",
    "        \n",
    "        # Map the words to their coefficients\n",
    "        pos_coeffs_dict = dict(zip(list(pos_word_weights.columns), pos_model.params))\n",
    "        pos_coeffs = pd.DataFrame(list(pos_coeffs_dict.items()), columns=['word','weight'])\n",
    "        neg_coeffs_dict = dict(zip(list(neg_word_weights.columns), neg_model.params))\n",
    "        neg_coeffs = pd.DataFrame(list(neg_coeffs_dict.items()), columns=['word','weight'])\n",
    "    \n",
    "        # Calculate the average word weight as well as the standard deviation\n",
    "        pos_avg = pos_coeffs['weight'].mean()\n",
    "        pos_std = pos_coeffs['weight'].std()\n",
    "        neg_avg = neg_coeffs['weight'].mean()\n",
    "        neg_std = neg_coeffs['weight'].std()\n",
    "        #print(\"Average: \" + str(pos_avg) + \"; StdDev: \" + str(pos_std))\n",
    "        #print(\"Average: \" + str(neg_avg) + \"; StdDev: \" + str(neg_std))\n",
    "        #print(pos_coeffs)\n",
    "\n",
    "        # Normalize the weights of the words\n",
    "        pos_norm = list()\n",
    "        for col, series in pos_coeffs.iteritems():\n",
    "            if col == 'weight':\n",
    "                for weight in series:\n",
    "                    pos_norm.append((weight - pos_avg) / pos_std)\n",
    "        pos_coeffs['norm_weight'] = pd.Series(pos_norm, index=pos_coeffs.index)\n",
    "        \n",
    "        neg_norm = list()\n",
    "        for col, series in neg_coeffs.iteritems():\n",
    "            if col == 'weight':\n",
    "                for weight in series:\n",
    "                    neg_norm.append((weight - neg_avg) / neg_std)\n",
    "        neg_coeffs['norm_weight'] = pd.Series(neg_norm, index=neg_coeffs.index)\n",
    "        \n",
    "        # Iterate through the original word weights and apply the normalized weight\n",
    "        for word, series in pos_word_weights.iteritems():\n",
    "            norm_weight = pos_coeffs.loc[pos_coeffs['word'] == word]['norm_weight']\n",
    "            pos_word_weights[word] = series.apply(lambda x: x * norm_weight)\n",
    "        for word, series in neg_word_weights.iteritems():\n",
    "            norm_weight = neg_coeffs.loc[neg_coeffs['word'] == word]['norm_weight']\n",
    "            neg_word_weights[word] = series.apply(lambda x: x * norm_weight)\n",
    "                \n",
    "        # Run the regression for abnormal (after filing) returns using the estimated weights for the words\n",
    "        pos_ab_reg = sm.OLS(ab_returns_series, pos_word_weights)\n",
    "        pos_ab_model = pos_ab_reg.fit()\n",
    "        neg_ab_reg = sm.OLS(ab_returns_series, neg_word_weights)\n",
    "        neg_ab_model = neg_ab_reg.fit()\n",
    "        \n",
    "        # Map the words to their coefficients\n",
    "        pos_coeffs_dict = dict(zip(list(pos_word_weights.columns), pos_ab_model.params))\n",
    "        pos_coeffs = pd.DataFrame(list(pos_coeffs_dict.items()), columns=['word','weight'])\n",
    "        neg_coeffs_dict = dict(zip(list(neg_word_weights.columns), neg_ab_model.params))\n",
    "        neg_coeffs = pd.DataFrame(list(neg_coeffs_dict.items()), columns=['word','weight'])\n",
    "\n",
    "        # Calculate the score of each document using the weights for each word given by the regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This will store the global positive and negative words occurrances\n",
    "pos_occurs_all = defaultdict(int)\n",
    "neg_occurs_all = defaultdict(int)\n",
    "\n",
    "# Go through redis and grab every 10-k\n",
    "keys = rds.keys(\"report:*\")\n",
    "\n",
    "print(\"Total keys: \" + str(len(keys)))\n",
    "for key in keys:\n",
    "    report_pos_occurs = pickle.loads(rds.hget(key, 'pos_occurs'))\n",
    "    report_neg_occurs = pickle.loads(rds.hget(key, 'neg_occurs'))\n",
    "    \n",
    "    for word, freq in report_pos_occurs.items():\n",
    "        pos_occurs_all[word] += freq\n",
    "        \n",
    "    for word, freq in report_neg_occurs.items():\n",
    "        neg_occurs_all[word] += freq\n",
    "\n",
    "# Print out most frequent positive words\n",
    "print(\"Most Frequent Positive Words\\n\" +\n",
    "       \"============================\")\n",
    "\n",
    "pos_sorted = pd.Series(data=pos_occurs_all).sort_values(ascending=False)\n",
    "print(pos_sorted)\n",
    "\n",
    "# Print out most frequent negative words\n",
    "print(\"\\n\\nMost Frequent Negative Words\\n\" +\n",
    "       \"============================\")\n",
    "\n",
    "neg_sorted = pd.Series(data=neg_occurs_all).sort_values(ascending=False)\n",
    "print(neg_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
