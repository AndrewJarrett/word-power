{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Power (Jupyter Notebook)\n",
    "The purpose of this Jupyter Notebook was to provide a scratch space where we could rapidly prototype our code and experiment in order to make sure our code was working properly. It is possible to run this code in order to get results, but this code has been moved into a separate program that can leverage multiprocessing algorithms and should be much more performant. \n",
    "\n",
    "## System Requirements\n",
    "Given that Jupyter Notebook/Python and Redis will need to load the data into system memory (RAM) the program can get very memory intense. It is necessary to at least have **16GB of RAM** in order to run the full program. In order to run this program, it is necessary to have Redis installed and working properly. If you are using Windows, Redis can be installed through [Chocolatey](https://chocolatey.org/) via `C:> choco install redis-64`. If you are running MacOSX you can install Redis via `$ brew install redis`. If you are running Linux or another Unix you should be able to install Redis through your package manager or compile from source.\n",
    "\n",
    "## Dependencies\n",
    "This program depends on having the necessary software and packages in order to run. First, you need to have `Python 3.5.2` installed. Next, you should be able to install all Python software dependencies through running `$ pip install -r requirements.txt`. In order to get the `lxml` package installed on Windows, it may be necessary to install the `.whl` file located in the `lib` project directory via `C:> pip install lib/lxml-3.6.4-cp35-cp35m-win_amd64.whl`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "We load the Cython extension in order to gain a little more performance out of some of the Python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ajarrett3\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\Cython\\Distutils\\old_build_ext.py:30: UserWarning: Cython.Distutils.old_build_ext does not properly handle dependencies and is deprecated.\n",
      "  \"Cython.Distutils.old_build_ext does not properly handle dependencies \"\n"
     ]
    }
   ],
   "source": [
    "%load_ext cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "This is where all of the necessary packages/imports are added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame, read_sas, read_csv\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from SECEdgar.crawler import SecCrawler\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "from datetime import date, timedelta\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import os\n",
    "import re\n",
    "import lxml\n",
    "import redis\n",
    "import string\n",
    "import pickle\n",
    "import math\n",
    "import zlib\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "try: stopwords.words('english')\n",
    "except LookupError: nltk.download('stopwords')\n",
    "    \n",
    "import statsmodels.api as sm\n",
    "    \n",
    "rds = redis.Redis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "This cell will check Redis in order to try to load data (CRSP and Compustat in addition to several dictionaries used later). The `crsp_comp.sas7bdat` file is generated from the `CRSP+Comp.sas` file which needs to be executed via SAS in order to get the necessary stock and accounting information. The `crsp_comp.sas7bdat` file needs to be placed in the `data/` folder of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in SAS data set - takes a while so try to use redis...\n",
    "try: data\n",
    "except NameError:\n",
    "    if rds.exists('data:word-power'):\n",
    "        %time data = pickle.loads(zlib.decompress(rds.get('data:word-power')))\n",
    "    else:\n",
    "        %time data = read_sas(\"../data/crsp_comp.sas7bdat\")\n",
    "\n",
    "        # Trim the SAS data set\n",
    "        data = data[['CUSIP','PERMNO','cik','tic','date','PRC','RET','vwretd']]\n",
    "\n",
    "        # Sort the set by cusip, permno, cik, and then year (descending)\n",
    "        data.sort_values(['CUSIP', 'PERMNO', 'cik', 'date'], ascending=[True, True, True, False], inplace=True)\n",
    "\n",
    "        # Re-index the dataframe for better access\n",
    "        data.reset_index(inplace=True)\n",
    "\n",
    "        rds.set('data:word-power', zlib.compress(pickle.dumps(data)))\n",
    "\n",
    "# We only need certain columns from the data set and we must set the right index for performance\n",
    "try: df\n",
    "except NameError:\n",
    "    df = data[[\"cik\", \"date\", \"PRC\", \"RET\", \"vwretd\"]]\n",
    "    df.set_index(keys=['cik','date'], inplace=True)\n",
    "    \n",
    "# Positive words\n",
    "try: pos_dict, pos_roots, pos_roots_map\n",
    "except NameError:\n",
    "    if rds.exists('data:pos-dict') and rds.exists('data:pos-roots') and rds.exists('data:pos-roots-map'):\n",
    "        pos_dict = pickle.loads(zlib.decompress(rds.get('data:pos-dict')))\n",
    "        pos_roots = pickle.loads(zlib.decompress(rds.get('data:pos-roots')))\n",
    "        pos_roots_map = pickle.loads(zlib.decompress(rds.get('data:pos-roots-map')))\n",
    "    else:\n",
    "        # Read in the positive word list(s)\n",
    "        pos_dict = read_csv(\"../data/pos_list.csv\", header=None, names=['word'])\n",
    "        pos_dict = set(pos_dict['word'])\n",
    "        pos_roots = read_csv(\"../data/pos_roots.csv\")\n",
    "        pos_roots_map = dict(zip(list(pos_roots.word), list(pos_roots.group)))\n",
    "        pos_roots = set(pos_roots['group'].drop_duplicates())\n",
    "\n",
    "        # Save this data to redis for later\n",
    "        rds.set('data:pos-dict', zlib.compress(pickle.dumps(pos_dict)))\n",
    "        rds.set('data:pos-roots', zlib.compress(pickle.dumps(pos_roots)))\n",
    "        rds.set('data:pos-roots-map', zlib.compress(pickle.dumps(pos_roots_map)))\n",
    "\n",
    "# Negative words\n",
    "try: neg_dict, neg_roots, neg_roots_map\n",
    "except NameError:\n",
    "    if rds.exists('data:neg-dict') and rds.exists('data:neg-roots') and rds.exists('data:neg-roots-map'):\n",
    "        neg_dict = pickle.loads(zlib.decompress(rds.get('data:neg-dict')))\n",
    "        neg_roots = pickle.loads(zlib.decompress(rds.get('data:neg-roots')))\n",
    "        neg_roots_map = pickle.loads(zlib.decompress(rds.get('data:neg-roots-map')))\n",
    "    else:\n",
    "        # Read in the negative word list(s)\n",
    "        neg_dict = read_csv(\"../data/neg_list.csv\", header=None, names=['word'])\n",
    "        neg_dict = set(neg_dict['word'])\n",
    "        neg_roots = read_csv(\"../data/neg_roots.csv\")\n",
    "        neg_roots_map = dict(zip(list(neg_roots.word), list(neg_roots.group)))\n",
    "        neg_roots = set(neg_roots['group'].drop_duplicates())\n",
    "\n",
    "        # Save this data to redis for later\n",
    "        rds.set('data:neg-dict', zlib.compress(pickle.dumps(neg_dict)))\n",
    "        rds.set('data:neg-roots', zlib.compress(pickle.dumps(neg_roots)))\n",
    "        rds.set('data:neg-roots-map', zlib.compress(pickle.dumps(neg_roots_map)))\n",
    "\n",
    "# 2of12inf dictionary\n",
    "try: dict_2of12inf\n",
    "except NameError:\n",
    "    if rds.exists('data:2of12inf'):\n",
    "        dict_2of12inf = pickle.loads(zlib.decompress(rds.get('data:2of12inf')))\n",
    "    else:\n",
    "        # Read in the 2of12inf\n",
    "        dict_2of12inf = read_csv(\"../data/2of12inf.txt\", header=None, names=['word'])\n",
    "\n",
    "        # Iterate through and remove the percent signs\n",
    "        regex = re.compile(r'%$')\n",
    "        dict_2of12inf.apply(lambda x: re.sub(regex, r'', x['word']), axis=1)\n",
    "        dict_2of12inf = set(dict_2of12inf['word'])\n",
    "\n",
    "        # Save this to redis for later\n",
    "        rds.set('data:2of12inf', zlib.compress(pickle.dumps(dict_2of12inf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "## 10-K Scraping from SEC Edgar\n",
    "This cell will initiate the 10-K scraping algorithm using the `SECEdgar` library for Python. This will take a long time to run given that it downloads the past 20 files up until 2008. This library doesn't offer much customization and it would be better to implement our own scraper if we had enough time so we didn't have to download the number of years in multiples of 10. We would also be able to check if the file is a 10-K/A vs. a 10-K before we downloaded it as well which would save some time. We did have to make some modifications to this third-party package so it is included in the `lib/` folder of the project if the default package code does not work for you. The crawler will download the file to disk and place it in a folder format like ```\"SEC-Edgar-data/\" + tic + \"/\" + cik + \"/\" + file```. You will need about **50GB of free space** on your disk in order to download all of the files in the date range 1995 - 2008."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['tic'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-5ca476b79039>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;31m# Only keep the cik and ticker column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mciks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mciks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cik'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tic'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[1;31m# Iterate over each CIK and pull the relevant 10k filings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\users\\ajarrett3\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2051\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2052\u001b[0m             \u001b[1;31m# either boolean or fancy integer index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2053\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2054\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2055\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\users\\ajarrett3\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2095\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2096\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2097\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2098\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2099\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\users\\ajarrett3\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[1;32m   1227\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m                     \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%s not in index'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mobjarr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0m_values_from_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['tic'] not in index\""
     ]
    }
   ],
   "source": [
    "#%%capture\n",
    "\n",
    "# Remove any duplicates where CUSIP, PERMNO, and CIK match\n",
    "ciks = data.drop_duplicates(subset=['CUSIP', 'PERMNO', 'cik'])\n",
    "\n",
    "# Only keep the cik and ticker column\n",
    "ciks = ciks[['cik', 'tic']]\n",
    "\n",
    "# Iterate over each CIK and pull the relevant 10k filings\n",
    "crawler = SecCrawler()\n",
    "end_date = '20081231'\n",
    "count = '20'\n",
    "\n",
    "for index, row in ciks.iterrows():\n",
    "    cik = row.iloc[0]\n",
    "    tic = row.iloc[1]\n",
    "    crawler.filing_10K(tic, cik, end_date, count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['tic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "## Check Redis Function\n",
    "This cell contains the `check_redis` function, which will check Redis in order to see if a given 10-K has been processed. When we later process a file, we will log this in Redis for easy checking in order to make sure all files are processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "from cpython cimport bool\n",
    "\n",
    "from __main__ import rds\n",
    "\n",
    "def check_redis(str cleaned_key, str processed_key, str report_key):\n",
    "    cdef bool processed = False\n",
    "    cdef bool cleaned = False\n",
    "    cdef str mtime\n",
    "    \n",
    "    if not rds.exists(cleaned_key):\n",
    "        if not rds.exists(processed_key):\n",
    "            # Temporary check to see if this file has been processed fully\n",
    "            if rds.exists(report_key):\n",
    "                mtime = rds.hget(report_key, 'mtime')\n",
    "\n",
    "                if not rds.hexists(report_key, 'company_data'):\n",
    "                    # Hasn't been cleaned with the new algorithm, so keep booleans False\n",
    "                    pass\n",
    "                elif rds.hexists(report_key, 'hist_ret'):\n",
    "                    processed = True\n",
    "                    cleaned = True\n",
    "\n",
    "                    # Save to proper place in redis\n",
    "                    rds.set(cleaned_key, mtime)\n",
    "                    rds.set(processed_key, mtime)\n",
    "                else:\n",
    "                    cleaned = True\n",
    "\n",
    "                    # Save to proper place in redis\n",
    "                    rds.set(cleaned_key, mtime)\n",
    "        else:\n",
    "            processed = True\n",
    "    else:\n",
    "        # Check to see if this has really been cleaned (company_data exists)\n",
    "        if rds.hexists(report_key, 'company_data'):\n",
    "            cleaned = True\n",
    "            if rds.exists(processed_key):\n",
    "                processed = True\n",
    "        \n",
    "    return (cleaned, processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move File Function\n",
    "This cell contains the `move_file` function that will move a file to a given folder when a certain condition is met. This condition can be that the file is outside of our date range (mainly because it is before 1995), it is an amended 10-K, we encountered an error trying to process the file, or we do not have any stock data for the company when the 10-K was filed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "import os\n",
    "\n",
    "def move_file(fh, str fn, str folder, str tic, str cik, str filename, str message):\n",
    "    # Generate the new name of the file\n",
    "    cdef str s = os.sep\n",
    "    cdef str new_name = 'data' + s + folder + s + tic + '-' + cik + '-' + filename\n",
    "\n",
    "    # Close the file so that we can move it\n",
    "    fh.close()\n",
    "    os.rename(fn, new_name)\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Function\n",
    "This cell contains the `clean` function which is the workhorse function that loads a given 10-K file, finds the filing date, verifys we can process the file, strips out HTML and other formatting, and then filters out words that aren't in our dictionary. The resulting cleaned file is stored in Redis for later retrieval so this step doesn't have to be run again for a file once it has been cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "import pickle\n",
    "from datetime import datetime as dt\n",
    "from lxml import etree\n",
    "from io import StringIO\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "from __main__ import move_file, df, stopwords, rds, dict_2of12inf\n",
    "\n",
    "from cpython cimport bool\n",
    "\n",
    "regex = re.compile(r\"(.*\\.sgml\\s+?:\\s+?|.*\\nFILED AS OF DATE:\\s+?)([\\d]+?)\\n.*\", re.S)\n",
    "\n",
    "# This function handles the cleaning of the 10-K\n",
    "def clean(str fn):\n",
    "    cdef bool error = False\n",
    "    \n",
    "    cdef str s = os.sep\n",
    "    cdef str tic = fn.split(s)[1]\n",
    "    cdef str cik = fn.split(s)[2]\n",
    "    cdef str filename = fn.split(s)[4]\n",
    "    cdef str report_key = \"report:\" + cik + \":\" + fn\n",
    "    cdef str cleaned_key = \"cleaned:\" + cik + \":\" + fn\n",
    "\n",
    "    # Open the file, get all of the content, and then pull it into a parser\n",
    "    fh = open(fn, 'r')\n",
    "    cdef unicode contents = fh.read()\n",
    "\n",
    "    # Clean up some of the text to fix malformed HTML before parsing it\n",
    "    cdef list malformed_tags = ['ACCEPTANCE-DATETIME', 'TYPE', 'SEQUENCE', 'FILENAME', 'DESCRIPTION']\n",
    "    cdef str tag\n",
    "    for tag in malformed_tags:\n",
    "        # Do a regex that replaces all of these malformed tags in the document\n",
    "        regex = re.compile(r\"(\\n<%s>[^<]*?)\\n\" % re.escape(tag), re.I)\n",
    "        contents = regex.sub(r\"\\1</%s>\\n\" % tag, contents)\n",
    "\n",
    "    # Create the parser\n",
    "    parser = etree.HTMLParser()\n",
    "    document = etree.parse(StringIO(contents), parser)\n",
    "    doc = document.getroot()\n",
    "    \n",
    "    # The document can either have a root node of sec-document or ims-document\n",
    "    if doc.xpath('//sec-document') is not None:\n",
    "        root = doc.xpath('//sec-document[1]')[0]\n",
    "    elif doc.xpath('//ims-document') is not None: \n",
    "        root = doc.xpath('//ims-document[1]')[0]\n",
    "    elif doc.xpath('//document') is not None:\n",
    "        root = doc.xpath('//document[1]')[0]\n",
    "    elif doc.xpath('//error') is not None:\n",
    "        root = None\n",
    "    else:\n",
    "        root = None\n",
    "        \n",
    "    if root is None:\n",
    "        # Root node error \n",
    "        move_file(fh, fn, \"_error\", tic, cik, filename, \"No root or erroneous root node - moved file\")\n",
    "        error = True\n",
    "    if error: return error\n",
    "\n",
    "    # Check if this is an amended 10-K and throw it out if so\n",
    "    type_text = root.xpath('//type/text()')\n",
    "    if type_text is None or len(type_text) == 0:\n",
    "        move_file(fh, fn, \"_error\", tic, cik, filename, \"Error finding type - moved file\")\n",
    "        error = True\n",
    "    elif type_text[0] == '10-K/A':\n",
    "        move_file(fh, fn, \"_amended\", tic, cik, filename, \"Amended 10-K - moved file\")\n",
    "        error = True\n",
    "    if error: return error\n",
    "\n",
    "    # Get the 'acceptance-datetime' metadata element\n",
    "    acc_dt = root.xpath('//acceptance-datetime/text()')\n",
    "    if acc_dt is None or len(acc_dt) == 0:\n",
    "        header_text = None\n",
    "        # If we didn't find an <acceptance-datetime /> element, find the date elsewhere\n",
    "        if len(root.xpath('//sec-header/text()')) != 0:\n",
    "            header_text = root.xpath('//sec-header/text()')[0]\n",
    "        elif len(root.xpath('//ims-header/text()')) != 0:\n",
    "            header_text = root.xpath('//ims-header/text()')[0]\n",
    "\n",
    "        if header_text:\n",
    "            filing_dt_text = re.sub(regex, r\"\\2\", header_text)\n",
    "        else:\n",
    "            move_file(fh, fn, \"_error\", tic, cik, filename, \"Bad filing date - moved file\")\n",
    "            error = True\n",
    "        if error: return error\n",
    "    else:\n",
    "        # Get the filing date\n",
    "        filing_dt_text = acc_dt[0].split('\\n', 1)[0][:8]\n",
    "\n",
    "    filing_dt = dt.strptime(filing_dt_text, '%Y%m%d')\n",
    "    filing_ts = time.mktime(filing_dt.timetuple())\n",
    "    begin_dt = dt(1995, 1, 1)\n",
    "\n",
    "    # If the filing date is not within our date range, then move it\n",
    "    if begin_dt > filing_dt:\n",
    "        move_file(fh, fn, \"_outofrange\", tic, cik, filename, \"Out of date range - moved file.\")\n",
    "        error = True\n",
    "    if error: return error\n",
    "\n",
    "    # See if we can find stock info for this company on the filing date of the 10-K\n",
    "    cdef int index = 0\n",
    "    cik_df = None\n",
    "    try:\n",
    "        index = df.index.get_loc((bytes(cik, 'utf-8'), filing_dt))\n",
    "        cik_df = df.ix[bytes(cik, 'utf-8')]\n",
    "        price = cik_df.ix[filing_dt, 'PRC']\n",
    "        # Now, check if the price of the stock is less than $3.00\n",
    "        if price < 3.0:\n",
    "            move_file(fh, fn, \"_nostockdata\", tic, cik, filename, \"Price less than $3.00 - moved file.\")\n",
    "            error = True\n",
    "    except (IndexError, KeyError):\n",
    "        # We couldn't find the cik or date for this 10-k\n",
    "        move_file(fh, fn, \"_nostockdata\", tic, cik, filename, \"No stock data found - moved file.\")\n",
    "        error = True\n",
    "    if error: return error\n",
    "    \n",
    "    # Grab the report\n",
    "    cdef str report = ''.join(root.xpath('//document/text')[0].itertext())\n",
    "\n",
    "    # We will tokenize the text and iterate through each word\n",
    "    cdef list tokens = report.split()\n",
    "    cdef list keep_tokens = []\n",
    "    cdef set stopwords_set = set(stopwords.words('english'))\n",
    "    punc_table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    \n",
    "    # Filter out words\n",
    "    cdef str word\n",
    "    for word in tokens:\n",
    "        # Quick check to make sure we should keep filtering the word\n",
    "        if len(word) != 1:\n",
    "            # Strip punctuation from the word first and make it lowercase\n",
    "            word = word.translate(punc_table).lower()\n",
    "\n",
    "            # Add the word to the keep pile if it is not a stopword and if it is in 2of12inf dictionary\n",
    "            if word not in stopwords_set and word in dict_2of12inf:\n",
    "                keep_tokens.append(word)\n",
    "            \n",
    "    tokens = keep_tokens\n",
    "    report = \" \".join(tokens)\n",
    "    cdef int total_words = len(tokens)\n",
    "\n",
    "    # Gather info for report to save into redis\n",
    "    report_hash = {\n",
    "        'cik': cik,\n",
    "        'tic': tic,\n",
    "        'path': fn,\n",
    "        'file_name': filename,\n",
    "        'filing_date': filing_ts,\n",
    "        'year': filing_dt.year,\n",
    "        'report': report,\n",
    "        'total_words': total_words,\n",
    "        'company_data': pickle.dumps(cik_df),\n",
    "        'index': index,\n",
    "        'mtime': time.time()\n",
    "    }\n",
    "\n",
    "    # Close the file handle\n",
    "    fh.close()\n",
    "    \n",
    "    # Save the stuff to redis\n",
    "    print(\"Saving to redis: \" + report_key)\n",
    "    rds.hmset(report_key, report_hash)\n",
    "    rds.set(cleaned_key, time.time())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Function\n",
    "The `process` function will take the cleaned file, count occurrences of positive and negative words, and calculate the historical and abnormal returns for the company based on the filing date of the 10-K. This information is stored in Redis for later access when running our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "import os\n",
    "import pickle\n",
    "import math\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from __main__ import df, rds, pos_dict, pos_roots, pos_roots_map, neg_dict, neg_roots, neg_roots_map\n",
    "\n",
    "from cpython cimport bool\n",
    "\n",
    "def process(str fn):\n",
    "    \n",
    "    cdef str s = os.sep\n",
    "    cdef str tic = fn.split(s)[1]\n",
    "    cdef str cik = fn.split(s)[2]\n",
    "    cdef str report_key = \"report:\" + cik + \":\" + fn\n",
    "    cdef str processed_key = \"processed:\"+ cik + \":\" + fn\n",
    "    \n",
    "    # Get the report out of redis\n",
    "    #print(\"Found in redis: \" + report_key)\n",
    "    cdef str report = str(rds.hget(report_key, 'report'))\n",
    "    filing_dt = dt.fromtimestamp(int(float(rds.hget(report_key, 'filing_date').decode('utf-8'))))\n",
    "    cik_df = pickle.loads(rds.hget(report_key, 'company_data'))\n",
    "    cdef int index = int(rds.hget(report_key, 'index'))\n",
    "    cdef dict report_hash = {}\n",
    "\n",
    "    # Now that everything is cleaned up, we can run the word processing algorithm\n",
    "    pos_occurs = defaultdict(int)\n",
    "    neg_occurs = defaultdict(int)\n",
    "    negators = pd.Series(['not', 'no', 'never'])\n",
    "\n",
    "    # We will tokenize the text and iterate through each word\n",
    "    tokens = pd.Series(report.split())\n",
    "\n",
    "    # Now, process the text\n",
    "    cdef int i\n",
    "    cdef str token, root, word\n",
    "    cdef bool negated\n",
    "    for i, token in tokens.iteritems():\n",
    "        if token in pos_dict:\n",
    "            # Check to see if there is a negator\n",
    "            negated = False\n",
    "            for word in tokens.iloc[(i - 3):(i + 3)]:\n",
    "                if word in negators.values:\n",
    "                    #print(\"Found a negator: \" + word + \" - \" + token)\n",
    "                    negated = True\n",
    "            if not negated:\n",
    "                root = pos_roots_map[token]\n",
    "                pos_occurs[root] += 1\n",
    "        elif token in neg_dict:\n",
    "            # Check to see if there is a negator\n",
    "            negated = False\n",
    "            for word in tokens.iloc[(i - 3):(i + 3)]:\n",
    "                if word in negators.values:\n",
    "                    #print(\"Found a negator: \" + word + \" - \" + token)\n",
    "                    negated = True\n",
    "            if not negated:\n",
    "                root = neg_roots_map[token]\n",
    "                neg_occurs[root] += 1\n",
    "\n",
    "    # For the roots we didn't find, set frequency to zero\n",
    "    for root in pos_roots:\n",
    "        if root not in pos_occurs:\n",
    "            pos_occurs[root] = 0\n",
    "    for root in neg_roots:\n",
    "        if root not in neg_occurs:\n",
    "            neg_occurs[root] = 0\n",
    "            \n",
    "    # Use the index we found earlier to grab the historical info\n",
    "    hist_returns = cik_df.ix[(index + 1):, 'RET']\n",
    "\n",
    "    # Calculate the historical return before the filing date\n",
    "    cdef float hist_ret = 1.0\n",
    "    for col, series in hist_returns.iteritems():\n",
    "        if col == 'RET':\n",
    "            for r in series:\n",
    "                if not math.isnan(r):\n",
    "                    hist_ret *= (r + 1.0)\n",
    "    hist_ret = hist_ret - 1.0\n",
    "    #print(\"Historical return: \" + str(hist_ret))\n",
    "\n",
    "    # Use the index we found earlier to grab the four day window returns\n",
    "    returns = cik_df.ix[(index - 3):(index + 1), ['RET','vwretd']]\n",
    "\n",
    "    # Calculate the abnormal return: r_i = M{t=0, 3} (ret_i,j) - M{t=0,3} (ret_vwi,t)\n",
    "    cdef float ret = 1.0\n",
    "    cdef float ret_vwi = 1.0\n",
    "    for col, series in returns.iteritems():\n",
    "        if col == 'RET':\n",
    "            for r in series:\n",
    "                if not math.isnan(r):\n",
    "                    ret *= (r + 1.0)\n",
    "        elif col == 'vwretd':\n",
    "            for r in series:\n",
    "                if  not math.isnan(r):\n",
    "                    ret_vwi *= (r + 1.0)\n",
    "    cdef float ab_ret = ((ret - 1.0) - (ret_vwi - 1.0))\n",
    "    #print(\"Abnormal return: \" + str(ab_ret))\n",
    "\n",
    "    # Save results of text processing to key in redis\n",
    "    report_hash['pos_occurs'] = pickle.dumps(pos_occurs)\n",
    "    report_hash['neg_occurs'] = pickle.dumps(neg_occurs)\n",
    "    report_hash['hist_ret'] = hist_ret\n",
    "    report_hash['ab_ret'] = ab_ret\n",
    "    report_hash['mtime'] = time.time()\n",
    "\n",
    "    print(\"Saving to redis: \" + report_key)\n",
    "    rds.hmset(report_key, report_hash)\n",
    "    rds.set(processed_key, time.time())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Processing Algorithm\n",
    "This cell contains the code that will go through every 10-K file that was downloaded by the scraper, check if it has been processed or cleaned, clean it, and then process it. This will take a while to run, especially if you are processing all 10-Ks from 1995 to 2008."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "import os\n",
    "\n",
    "from __main__ import check_redis, clean, process\n",
    "\n",
    "from cpython cimport bool\n",
    "\n",
    "# This is for testing\n",
    "cdef int count = 1\n",
    "cdef int stop = 100000\n",
    "cdef bool skip_cleaned = True\n",
    "cdef bool skip_processed = True\n",
    "cdef bool process_file = True\n",
    "\n",
    "cdef dict report_hash\n",
    "cdef str fn, s, tic, cik, cleaned_key, processed_key, report_key\n",
    "cdef bool cleaned, processed, error\n",
    "\n",
    "cdef str dirpath\n",
    "cdef list dirnames, filenames\n",
    "cdef str folder = \"SEC-Edgar-data\"\n",
    "for (dirpath, dirnames, filenames) in os.walk(folder, topdown=False):\n",
    "    for filename in filenames:\n",
    "        report_hash = {}\n",
    "        fn = os.sep.join([dirpath, filename])\n",
    "        \n",
    "        if filename.endswith('.txt'):# and filename == \"0000950116-97-000637.txt\":\n",
    "            if count > stop:\n",
    "                break\n",
    "            print(fn)\n",
    "            s = os.sep\n",
    "            tic = fn.split(s)[1]\n",
    "            cik = fn.split(s)[2]\n",
    "            \n",
    "            # Check redis to see if we have processed or cleaned the report already\n",
    "            cleaned_key = \"cleaned:\" + cik + \":\" + fn\n",
    "            processed_key = \"processed:\" + cik + \":\" + fn\n",
    "            report_key = \"report:\" + cik + \":\" + fn\n",
    "            (cleaned, processed) = check_redis(cleaned_key, processed_key, report_key)\n",
    "            \n",
    "            # If the report has been cleaned or we don't want to clean it anyway, skip this step\n",
    "            error = False\n",
    "            if not cleaned or not skip_cleaned:\n",
    "                print(\"(\" + str(count) + \") Cleaning \" + fn)\n",
    "                error = clean(fn)\n",
    "                \n",
    "                if not process and not error:\n",
    "                    count += 1\n",
    "                    continue\n",
    "            if error: continue\n",
    "            \n",
    "            # After possibly cleaning, check if we should process the file\n",
    "            if (not processed or not skip_processed) and process_file:\n",
    "                print(\"(\" + str(count) + \") Processing \" + fn)\n",
    "                process(fn)\n",
    "                \n",
    "                count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Listing of Yearly Data\n",
    "This cell will gather data found in Redis and generate a listing of certain information about each 10-K report for its associated year. This information about the report will be used to run the regression analysis later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "stop = math.inf\n",
    "yearly_data = {}\n",
    "\n",
    "if rds.exists(\"yearly-data\"):\n",
    "    print(\"Found yearly data in Redis.\")\n",
    "    yearly_data = pickle.loads(zlib.decompress(rds.get(\"yearly-data\")))\n",
    "else:\n",
    "    keys = rds.keys(\"report:*\")\n",
    "    errors = []\n",
    "    for key in keys:\n",
    "\n",
    "        if count >= stop:\n",
    "            break\n",
    "\n",
    "        report_hash = rds.hgetall(key)\n",
    "        try:\n",
    "            year = 1\n",
    "            cik = str(report_hash[b'cik'].decode('utf-8'))\n",
    "            fn = str(report_hash[b'path'].decode('utf-8'))\n",
    "            key = \"report:\" + cik + \":\" + fn\n",
    "            pos_occurs = pickle.loads(report_hash[b'pos_occurs'])\n",
    "            neg_occurs = pickle.loads(report_hash[b'neg_occurs'])\n",
    "            year = int(report_hash[b'year'])\n",
    "            total_words = int(report_hash[b'total_words'])\n",
    "            hist_ret = float(report_hash[b'hist_ret'])\n",
    "            ab_ret = float(report_hash[b'ab_ret'])\n",
    "\n",
    "            if total_words == 0:\n",
    "                # Not sure why this would be zero, but we may need to reprocess\n",
    "                print(\"Error with: \" + key)\n",
    "                cleaned_key = \"cleaned:\" + cik + \":\" + fn\n",
    "                processed_key = \"processed:\" + cik + \":\" + fn\n",
    "\n",
    "                # Delete this error from redis\n",
    "                rds.delete(cleaned_key, processed_key, key)\n",
    "\n",
    "                errors.append(key)\n",
    "                continue\n",
    "\n",
    "            try: yearly_data[year]\n",
    "            except KeyError:\n",
    "                yearly_data[year] = []\n",
    "\n",
    "            year_list = yearly_data[year]\n",
    "            year_list.append({\n",
    "                'pos_occurs': pos_occurs,\n",
    "                'neg_occurs': neg_occurs,\n",
    "                'total_words': total_words,\n",
    "                'hist_ret': hist_ret,\n",
    "                'ab_ret': ab_ret\n",
    "            })\n",
    "            yearly_data[year] = year_list\n",
    "\n",
    "            count += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "        except e:\n",
    "            print(e)\n",
    "            \n",
    "    print(\"Saving yearly data in Redis.\")\n",
    "    rds.set(\"yearly-data\", zlib.compress(pickle.dumps(yearly_data)))\n",
    "    \n",
    "    if len(errors) > 0:\n",
    "        print(\"Total errors: \" + str(len(errors)))\n",
    "        \n",
    "for year in sorted(yearly_data.keys()):\n",
    "    print(year, len(yearly_data[year]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "## Regression Analysis\n",
    "This cell actually runs the regression analysis and will do a rolling analysis from the start year and end year. We realized too late that our CRSP and Compustat data didn't add in stock information for 1995, but this was corrected in the SAS code later. You should be able to run this analysis over the correct time frame. Also, we believe that this analysis should use 2009 as the end date in order to leverage the data from 2008 and make a prediction for the year 2009. We realized this too late and did not have time to rerun the regression analysis. These changes could affect our results and make them closer to the results from the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from __main__ import rds, yearly_data\n",
    "\n",
    "from cpython cimport bool\n",
    "\n",
    "cdef int start = 1997\n",
    "cdef int end = 2008\n",
    "cdef bool skip_regression = True\n",
    "cdef bool skip_building = True\n",
    "cdef int t\n",
    "cdef int year\n",
    "cdef int count\n",
    "cdef int total\n",
    "cdef str key\n",
    "\n",
    "# Clear some variables so we have enough RAM...\n",
    "try: del data, df\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "# Generate a rolling training model using data up until year T-1\n",
    "for t in range(start, (end + 1)):\n",
    "    \n",
    "    print(\"Analyzing year \" + str(t))\n",
    "    \n",
    "    # Skip this analysis if redis already has it and the boolean is set to skip\n",
    "    if rds.exists(\"regression:\" + str(t)) and skip_regression:\n",
    "        continue\n",
    "\n",
    "    pos_word_weights = pd.DataFrame()\n",
    "    neg_word_weights = pd.DataFrame()\n",
    "    hist_returns = pd.DataFrame()\n",
    "    ab_returns = pd.DataFrame()\n",
    "    \n",
    "    # Iterate over each year before year T and build the training data set\n",
    "    for year in range((start - 1), t):\n",
    "        \n",
    "        key = \"regression-data:\" + str(year)\n",
    "        \n",
    "        if rds.exists(key) and skip_building:\n",
    "            # Extract the data\n",
    "            pos_word_weights = pickle.loads(rds.hget(key, 'pos_word_weights'))\n",
    "            neg_word_weights = pickle.loads(rds.hget(key, 'neg_word_weights'))\n",
    "            hist_returns = pickle.loads(rds.hget(key, 'hist_returns'))\n",
    "            ab_returns = pickle.loads(rds.hget(key, 'ab_returns'))\n",
    "            continue\n",
    "        \n",
    "        print(\"Building year \" + str(year) + \"/\" + str(t))\n",
    "        \n",
    "        try: yearly_data[year]\n",
    "        except KeyError:\n",
    "            print(\"Year \" + str(year) + \" not found.\")\n",
    "            continue\n",
    "        \n",
    "        # Iterate through each 10-K info for the year and generate the dataframe for the regression\n",
    "        count = 0\n",
    "        total = len(yearly_data[year])\n",
    "        for report in yearly_data[year]:\n",
    "            count += 1\n",
    "            print(\"Report \" + str(count) + \"/\" + str(total))\n",
    "            \n",
    "            a = report['total_words']\n",
    "            hist_ret = report['hist_ret']\n",
    "            ab_ret = report['ab_ret']\n",
    "            \n",
    "            weights = {}\n",
    "            pos_occurs = report['pos_occurs']\n",
    "            for word in pos_occurs.keys():\n",
    "                F = pos_occurs[word]\n",
    "                weights[word] = F/(a * 1.0)\n",
    "            pos_word_weights = pos_word_weights.append(weights, ignore_index=True)\n",
    "            \n",
    "            weights = {}\n",
    "            neg_occurs = report['neg_occurs']\n",
    "            for word in neg_occurs.keys():\n",
    "                F = neg_occurs[word]\n",
    "                weights[word] = F/(a * 1.0)\n",
    "            neg_word_weights = neg_word_weights.append(weights, ignore_index=True)\n",
    "            \n",
    "            hist_returns = hist_returns.append({'hist_ret': hist_ret}, ignore_index=True)\n",
    "            ab_returns = ab_returns.append({'ab_ret': ab_ret}, ignore_index=True)\n",
    "            \n",
    "        # Save our progress so we don't have to recalculate this year again\n",
    "        print(\"Saving progress for year \" + str(year) + \" to redis.\")\n",
    "        reg_data_hash = {\n",
    "            'pos_word_weights': pickle.dumps(pos_word_weights),\n",
    "            'neg_word_weights': pickle.dumps(neg_word_weights),\n",
    "            'hist_returns': pickle.dumps(hist_returns),\n",
    "            'ab_returns': pickle.dumps(ab_returns)\n",
    "        }\n",
    "        rds.hmset(key, reg_data_hash)\n",
    "        \n",
    "    # Run the regressions for all years up to year T\n",
    "    if not ab_returns.empty and not hist_returns.empty and not pos_word_weights.empty and not neg_word_weights.empty:\n",
    "        hist_returns.reset_index()\n",
    "        hist_returns_series = pd.Series(hist_returns['hist_ret'])\n",
    "        ab_returns.reset_index()\n",
    "        ab_returns_series = pd.Series(ab_returns['ab_ret'])\n",
    "        pos_word_weights.reset_index()\n",
    "        neg_word_weights.reset_index()\n",
    "        \n",
    "        # Estimate the weights for the words using a regression\n",
    "        print(\"T = \" + str(t) + \": Estimating weights\")\n",
    "        pos_reg = sm.OLS(hist_returns_series, pos_word_weights)\n",
    "        pos_model = pos_reg.fit()\n",
    "        neg_reg = sm.OLS(hist_returns_series, neg_word_weights)\n",
    "        neg_model = neg_reg.fit()\n",
    "        \n",
    "        # Map the words to their coefficients\n",
    "        pos_coeffs_dict = dict(zip(list(pos_word_weights.columns), pos_model.params))\n",
    "        pos_coeffs = pd.DataFrame(list(pos_coeffs_dict.items()), columns=['word','weight'])\n",
    "        neg_coeffs_dict = dict(zip(list(neg_word_weights.columns), neg_model.params))\n",
    "        neg_coeffs = pd.DataFrame(list(neg_coeffs_dict.items()), columns=['word','weight'])\n",
    "    \n",
    "        # Calculate the average word weight as well as the standard deviation\n",
    "        pos_avg = pos_coeffs['weight'].mean()\n",
    "        pos_std = pos_coeffs['weight'].std()\n",
    "        neg_avg = neg_coeffs['weight'].mean()\n",
    "        neg_std = neg_coeffs['weight'].std()\n",
    "\n",
    "        # Normalize the weights of the words\n",
    "        print(\"T = \" + str(t) + \": Normalizing weights\")\n",
    "        pos_norm = list()\n",
    "        for col, series in pos_coeffs.iteritems():\n",
    "            if col == 'weight':\n",
    "                for weight in series:\n",
    "                    pos_norm.append((weight - pos_avg) / pos_std)\n",
    "        pos_coeffs['norm_weight'] = pd.Series(pos_norm, index=pos_coeffs.index)\n",
    "        \n",
    "        neg_norm = list()\n",
    "        for col, series in neg_coeffs.iteritems():\n",
    "            if col == 'weight':\n",
    "                for weight in series:\n",
    "                    neg_norm.append((weight - neg_avg) / neg_std)\n",
    "        neg_coeffs['norm_weight'] = pd.Series(neg_norm, index=neg_coeffs.index)\n",
    "        \n",
    "        # Iterate through the original word weights and apply the normalized weight\n",
    "        for word, series in pos_word_weights.iteritems():\n",
    "            norm_weight = pos_coeffs.loc[pos_coeffs['word'] == word]['norm_weight']\n",
    "            pos_word_weights[word] = series.apply(lambda x: x * norm_weight)\n",
    "        for word, series in neg_word_weights.iteritems():\n",
    "            norm_weight = neg_coeffs.loc[neg_coeffs['word'] == word]['norm_weight']\n",
    "            neg_word_weights[word] = series.apply(lambda x: x * norm_weight)\n",
    "                \n",
    "        # Run the regression for abnormal (after filing) returns using the estimated weights for the words\n",
    "        print(\"T = \" + str(t) + \": Doing regression\")\n",
    "        pos_ab_reg = sm.OLS(ab_returns_series, pos_word_weights)\n",
    "        pos_ab_model = pos_ab_reg.fit()\n",
    "        neg_ab_reg = sm.OLS(ab_returns_series, neg_word_weights)\n",
    "        neg_ab_model = neg_ab_reg.fit()\n",
    "        \n",
    "        # Map the words to their coefficients\n",
    "        pos_coeffs_dict = dict(zip(list(pos_word_weights.columns), pos_ab_model.params))\n",
    "        pos_coeffs = pd.DataFrame(list(pos_coeffs_dict.items()), columns=['word','weight'])\n",
    "        neg_coeffs_dict = dict(zip(list(neg_word_weights.columns), neg_ab_model.params))\n",
    "        neg_coeffs = pd.DataFrame(list(neg_coeffs_dict.items()), columns=['word','weight'])\n",
    "        \n",
    "        # Store the regression models in Redis\n",
    "        key = \"regression:\" + str(t)        \n",
    "        reg_hash = {\n",
    "            'pos_model': pickle.dumps(pos_model),\n",
    "            'neg_model': pickle.dumps(neg_model),\n",
    "            'pos_ab_model': pickle.dumps(pos_ab_model),\n",
    "            'neg_ab_model': pickle.dumps(neg_ab_model)\n",
    "        }        \n",
    "        rds.hmset(key, reg_hash)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Reports\n",
    "This cell contains the code that will run two reports. The first report is a \"matrix\" of both positive and negative term frequency grouped by their weight frequency. This is essentially our version of `Table II` from the Appendix of the *Word Power* paper. The second report is a listing of both positive and negative terms group by their term frequency (least frequently occurring to most frequently) and the top five words within those buckets are the ones with the highest coefficients from the regression analysis. Therefore, this is essentially listing out the most effective words (based on returns) grouped by their frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This will store the global positive and negative words occurrances\n",
    "pos_occurs_all = defaultdict(int)\n",
    "neg_occurs_all = defaultdict(int)\n",
    "\n",
    "try: yearly_data\n",
    "except NameError: yearly_data = pickle.loads(zlib.decompress(rds.get('yearly-data')))\n",
    "\n",
    "if rds.exists(\"freq:pos:all\") and rds.exists(\"freq:neg:all\"):\n",
    "    pos_occurs_all = pickle.loads(zlib.decompress(rds.get(\"freq:pos:all\")))\n",
    "    neg_occurs_all = pickle.loads(zlib.decompress(rds.get(\"freq:neg:all\")))\n",
    "else:\n",
    "    for year in yearly_data:\n",
    "        print(\"Counting frequency for year \" + str(year))\n",
    "\n",
    "        pos_occurs_year = defaultdict(int)\n",
    "        neg_occurs_year = defaultdict(int)\n",
    "        pos_key = \"freq:pos:\" + str(year)\n",
    "        neg_key = \"freq:neg:\" + str(year)\n",
    "\n",
    "        if rds.exists(pos_key) and rds.exists(neg_key):\n",
    "            pos_occurs_year = pickle.loads(zlib.decompress(rds.get(pos_key)))\n",
    "            neg_occurs_year = pickle.loads(zlib.decompress(rds.get(neg_key)))\n",
    "        else:\n",
    "            count = 0\n",
    "            total = len(yearly_data[year])\n",
    "            for report in yearly_data[year]:\n",
    "                count += 1\n",
    "                print(\"Counting Report \" + str(count) + \"/\" + str(total))\n",
    "                report_pos_occurs = report['pos_occurs']\n",
    "                report_neg_occurs = report['neg_occurs']\n",
    "\n",
    "                for word, freq in report_pos_occurs.items():\n",
    "                    pos_occurs_year[word] += freq\n",
    "                for word, freq in report_neg_occurs.items():\n",
    "                    neg_occurs_year[word] += freq\n",
    "\n",
    "            print(\"Saving progress to Redis.\")\n",
    "            rds.set(\"freq:pos:\" + str(year), zlib.compress(pickle.dumps(pos_occurs_year)))\n",
    "            rds.set(\"freq:neg:\" + str(year), zlib.compress(pickle.dumps(neg_occurs_year)))\n",
    "\n",
    "    for word, freq in pos_occurs_year.items():\n",
    "        pos_occurs_all[word] += freq\n",
    "    for word, freq in neg_occurs_year.items():\n",
    "        neg_occurs_all[word] += freq\n",
    "        \n",
    "    print(\"Finished totaling and now saving to Redis.\")\n",
    "    rds.set(\"freq:pos:all\", zlib.compress(pickle.dumps(pos_occurs_all)))\n",
    "    rds.set(\"freq:neg:all\", zlib.compress(pickle.dumps(neg_occurs_all)))\n",
    "\n",
    "# Sort the values by highest to lowest\n",
    "pos_sorted = pd.Series(data=pos_occurs_all).sort_values()\n",
    "neg_sorted = pd.Series(data=neg_occurs_all).sort_values()\n",
    "\n",
    "# Get the coefficients\n",
    "pos_coeffs = pickle.loads(zlib.decompress(rds.hget(\"regression:2008\", 'pos_coeffs')))\n",
    "neg_coeffs = pickle.loads(zlib.decompress(rds.hget(\"regression:2008\", 'neg_coeffs')))\n",
    "\n",
    "# Place the weights into quintiles\n",
    "pos_wt_buckets = pd.qcut(pos_coeffs['weight'], 5, labels=range(1,6))\n",
    "neg_wt_buckets = pd.qcut(neg_coeffs['weight'], 5, labels=range(1,6))\n",
    "\n",
    "# Sort the coefficients by value\n",
    "pos_coeffs.sort_values('weight', inplace=True)\n",
    "neg_coeffs.sort_values('weight', inplace=True)\n",
    "\n",
    "# See if we have the total 10-Ks and number of appearances of each word in a 10-k\n",
    "try: total, pos_appearances, neg_appearances\n",
    "except NameError:\n",
    "    (total, pos_appearances, neg_appearances) = get_appearances()\n",
    "\n",
    "# Place the frequencies into quintiles\n",
    "pos_freq_buckets = pd.qcut(pos_appearances['apps'], 5, labels=range(1,6))\n",
    "neg_freq_buckets = pd.qcut(neg_appearances['apps'], 5, labels=range(1,6))\n",
    "\n",
    "bucket_freq_list = []\n",
    "bucket_wt_list = []\n",
    "freq_list = []\n",
    "total_list = []\n",
    "for (word, row) in pos_coeffs.iterrows():\n",
    "    bucket_freq_list.append(pos_freq_buckets.ix[word])\n",
    "    bucket_wt_list.append(pos_wt_buckets.ix[word])\n",
    "    freq_list.append(pos_sorted.ix[word])\n",
    "    total_list.append(pos_appearances.ix[word]['apps'])\n",
    "pos_coeffs['bucket_freq'] = pd.Series(bucket_freq_list, index=pos_coeffs.index)\n",
    "pos_coeffs['bucket_weight'] = pd.Series(bucket_wt_list, index=pos_coeffs.index)\n",
    "pos_coeffs['freq'] = pd.Series(freq_list, index=pos_coeffs.index)\n",
    "pos_coeffs['apps'] = pd.Series(total_list, index=pos_coeffs.index)\n",
    "pos_coeffs['files'] = pos_appearances['files']\n",
    "\n",
    "bucket_freq_list = []\n",
    "bucket_wt_list = []\n",
    "freq_list = []\n",
    "total_list = []\n",
    "for (word, row) in neg_coeffs.iterrows():\n",
    "    bucket_freq_list.append(neg_freq_buckets.ix[word])\n",
    "    bucket_wt_list.append(neg_wt_buckets.ix[word])\n",
    "    freq_list.append(neg_sorted.ix[word])\n",
    "    total_list.append(neg_appearances.ix[word]['apps'])\n",
    "neg_coeffs['bucket_freq'] = pd.Series(bucket_freq_list, index=neg_coeffs.index)\n",
    "neg_coeffs['bucket_weight'] = pd.Series(bucket_wt_list, index=neg_coeffs.index)\n",
    "neg_coeffs['freq'] = pd.Series(freq_list, index=neg_coeffs.index)\n",
    "neg_coeffs['apps'] = pd.Series(total_list, index=neg_coeffs.index)\n",
    "neg_coeffs['files'] = neg_appearances['files']\n",
    "\n",
    "# Build the table of weight quintiles to frequency quintiles\n",
    "pos_tab5 = pos_coeffs.reset_index(level='word').set_index(['bucket_weight','bucket_freq','word'])\n",
    "neg_tab5 = neg_coeffs.reset_index(level='word').set_index(['bucket_weight','bucket_freq','word'])\n",
    "\n",
    "# Iterate first through the weight buckets\n",
    "print(\"Positive Weight Quintiles by Term Frequency Quintiles\")\n",
    "for i in range(1,6):\n",
    "    # Get the total number of words in this weight quantile\n",
    "    tot_in_quantile = len(pos_tab5.ix[i]['weight'])\n",
    "    \n",
    "    print(\"\\nWeight Quintile \" + str(i) + \": Total words = \" + str(tot_in_quantile))\n",
    "    \n",
    "    # Find the number of words in each freq bucket and show that as percentage of words in weight bucket\n",
    "    for j in range(1,6):\n",
    "        num_words = len(pos_tab5.ix[(i,j)]['weight'])\n",
    "        perc = (num_words / (1.0 * tot_in_quantile)) * 100\n",
    "        print(\"Term Freq Quintile \" + str(j) + \": \" + str(perc))\n",
    "        \n",
    "print(\"\\nNegative Weight Quintiles by Term Frequency Quintiles\")\n",
    "for i in range(1,6):\n",
    "    tot_in_quantile = len(neg_tab5.ix[i]['weight'])\n",
    "    \n",
    "    print(\"\\nWeight Quintile \" + str(i) + \": Total words = \" + str(tot_in_quantile))\n",
    "    \n",
    "    for j in range(1,6):\n",
    "        num_words = len(neg_tab5.ix[(i,j)]['weight'])\n",
    "        perc = (num_words / (1.0 * tot_in_quantile)) * 100\n",
    "        print(\"Term Freq Quintile \" + str(j) + \": \" + str(perc))\n",
    "\n",
    "\n",
    "# Now, sort by the frequency buckets (ascending) and weights (descending)\n",
    "pos_coeffs.sort_values(['bucket_freq','weight'], ascending=[True,False], inplace=True)\n",
    "neg_coeffs.sort_values(['bucket_freq','weight'], ascending=[True,False], inplace=True)\n",
    "\n",
    "# Get the top 5 words in each quintile\n",
    "pos_top5 = pos_coeffs.reset_index(level=['word']).set_index(['bucket_freq'])\n",
    "pos_top5_list = {}\n",
    "neg_top5 = neg_coeffs.reset_index(level=['word']).set_index(['bucket_freq'])\n",
    "neg_top5_list = {}\n",
    "for i in range(1, 6):\n",
    "    pos_words = []\n",
    "    pos_bucket_words = pos_top5.ix[i]['word']\n",
    "    neg_words = []\n",
    "    neg_bucket_words = neg_top5.ix[i]['word']\n",
    "    \n",
    "    count = 0\n",
    "    for word in pos_bucket_words:\n",
    "        if count >= 5:\n",
    "            break\n",
    "        pos_words.append(word)\n",
    "        count += 1\n",
    "    count = 0\n",
    "    for word in neg_bucket_words:\n",
    "        if count >= 5:\n",
    "            break\n",
    "        neg_words.append(word)\n",
    "        count += 1\n",
    "    \n",
    "    pos_top5_list[i] = pos_words\n",
    "    neg_top5_list[i] = neg_words\n",
    "\n",
    "# Print out most frequent positive words\n",
    "header = \"Top Five Most Positive and Negative Words within Frequency Quintiles\"\n",
    "print(header)\n",
    "print(\"=\"*len(header))\n",
    "\n",
    "subheader = \"\\nTop 5 Most Positive Words\"\n",
    "print(subheader)\n",
    "print(\"=\"*len(subheader))\n",
    "\n",
    "for (bucket, words) in pos_top5_list.items():\n",
    "    print(\"\\nBucket \" + str(bucket))\n",
    "    for word in words:\n",
    "        print(word)\n",
    "\n",
    "# Print out most frequent negative words\n",
    "subheader = \"\\nTop 5 Most Negative Words\"\n",
    "print(subheader)\n",
    "print(\"=\"*len(subheader))\n",
    "\n",
    "for (bucket, words) in neg_top5_list.items():\n",
    "    print(\"\\nBucket \" + str(bucket))\n",
    "    for word in words:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Appearances Function\n",
    "This cell contains the `get_appearances` function which is used above in generating the reports that show which words occur the most frequently. This function will count the number of appearances for each positive or negative word across all 10-Ks we are analyzing. Note that it will calculate this based on the total number of 10-Ks in which the word appears divided by the total number of 10-Ks that we processed in order to calculate the total percentage of 10-Ks in which the word appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function will get a count of the number of appearances for each positive or negative word across all 10-Ks.\n",
    "def get_appearances():\n",
    "    try: yearly_data\n",
    "    except NameError: yearly_data = pickle.loads(zlib.decompress(rds.get(\"yearly-data\")))\n",
    "\n",
    "    pos_appearances_dict = defaultdict(int)\n",
    "    neg_appearances_dict = defaultdict(int)\n",
    "    pos_app_set = {}\n",
    "    neg_app_set = {}    \n",
    "\n",
    "    total = 0\n",
    "    for year in yearly_data:\n",
    "        total += len(yearly_data[year])\n",
    "\n",
    "        i = 0\n",
    "        for report in yearly_data[year]:\n",
    "            # Keep track if a word appears at least once and increment a counter if so\n",
    "            pos_occurs = report['pos_occurs']\n",
    "            neg_occurs = report['neg_occurs']\n",
    "\n",
    "            for (word, freq) in pos_occurs.items():\n",
    "                try: word_set = pos_app_set[word]\n",
    "                except: word_set = set()\n",
    "                    \n",
    "                if freq > 0:\n",
    "                    pos_appearances_dict[word] += 1\n",
    "                    word_set.add(str(year) + \":\" + str(i))\n",
    "                    pos_app_set[word] = word_set\n",
    "                else:\n",
    "                    pos_appearances_dict[word] += 0\n",
    "                    pos_app_set[word] = word_set\n",
    "                    \n",
    "            for (word, freq) in neg_occurs.items():\n",
    "                if freq > 0:\n",
    "                    neg_appearances_dict[word] += 1\n",
    "                    word_set.add(str(year) + \":\" + str(i))\n",
    "                    neg_app_set[word] = word_set\n",
    "                else:\n",
    "                    neg_appearances_dict[word] += 0\n",
    "                    neg_app_set[word] = word_set\n",
    "            i += 1\n",
    "\n",
    "    pos_appearances = pd.DataFrame({'apps': pos_appearances_dict, 'files': pos_app_set})\n",
    "    neg_appearances = pd.DataFrame({'apps': neg_appearances_dict, 'files': neg_app_set})\n",
    "    return (total, pos_appearances, neg_appearances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
